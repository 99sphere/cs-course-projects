{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X50JgS10dybp"
   },
   "source": [
    "# SE395 Programming Assignment1\n",
    "#### 201811118 이  구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfwbpiq9ZDjp"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWnW-Ird4DUL"
   },
   "source": [
    "## 1. Prepare training/test dataset\n",
    "### 1.1 Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auHpfrGUd-N3"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "def training_images():\n",
    "    with gzip.open('train-images-idx3-ubyte.gz', 'r') as f:\n",
    "    #with gzip.open('/content/gdrive/My Drive/MNIST_DATASET/train-images-idx3-ubyte.gz', 'r') as f:\n",
    "        magic_number = int.from_bytes(f.read(4), 'big')\n",
    "        image_count = int.from_bytes(f.read(4), 'big')\n",
    "        row_count = int.from_bytes(f.read(4), 'big')\n",
    "        column_count = int.from_bytes(f.read(4), 'big')\n",
    "        image_data = f.read()\n",
    "        images = np.frombuffer(image_data, dtype=np.uint8).reshape((image_count, row_count, column_count))\n",
    "        return images\n",
    "\n",
    "def training_labels():\n",
    "    with gzip.open('train-labels-idx1-ubyte.gz', 'r') as f:\n",
    "        magic_number = int.from_bytes(f.read(4), 'big')\n",
    "        label_count = int.from_bytes(f.read(4), 'big')\n",
    "        label_data = f.read()\n",
    "        labels = np.frombuffer(label_data, dtype=np.uint8)\n",
    "        return labels\n",
    "\n",
    "def test_images():\n",
    "    with gzip.open('t10k-images-idx3-ubyte.gz', 'r') as f:\n",
    "        magic_number = int.from_bytes(f.read(4), 'big')\n",
    "        image_count = int.from_bytes(f.read(4), 'big')\n",
    "        row_count = int.from_bytes(f.read(4), 'big')\n",
    "        column_count = int.from_bytes(f.read(4), 'big')\n",
    "        image_data = f.read()\n",
    "        images = np.frombuffer(image_data, dtype=np.uint8).reshape((image_count, row_count, column_count))\n",
    "        return images\n",
    "        \n",
    "def test_labels():\n",
    "    with gzip.open('t10k-labels-idx1-ubyte.gz', 'r') as f:\n",
    "        magic_number = int.from_bytes(f.read(4), 'big')\n",
    "        label_count = int.from_bytes(f.read(4), 'big')\n",
    "        label_data = f.read()\n",
    "        labels = np.frombuffer(label_data, dtype=np.uint8)\n",
    "        return labels\n",
    "\n",
    "def make_flatten_arr(arr):\n",
    "    result = []\n",
    "    for i in range(len(arr)):\n",
    "        result.append(arr[i].flatten())\n",
    "    return np.array(result)\n",
    "\n",
    "def img_vis(X, Y, idx):\n",
    "    img = X[idx].reshape(28,28)\n",
    "    plt.imshow(img)\n",
    "    plt.title(str(Y[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "usg1gm6TfHRs",
    "outputId": "1ead150a-1d36-4051-80ee-5f415057dc76"
   },
   "outputs": [],
   "source": [
    "# for train dataset\n",
    "X_train = training_images()\n",
    "X_train = make_flatten_arr(X_train)\n",
    "Y_train = np.array(training_labels())\n",
    "\n",
    "# for test dataset\n",
    "X_test = test_images()\n",
    "X_test = make_flatten_arr(X_test)\n",
    "Y_test = test_labels()\n",
    "\n",
    "# check result\n",
    "print(\"--------- train set ---------\")\n",
    "print(\"Shape of X_train: \", X_train.shape)\n",
    "print(\"Shape of Y_train: \", Y_train.shape)\n",
    "print(\"\\n--------- test set ---------\")\n",
    "print(\"Shape of X_test: \", X_test.shape)\n",
    "print(\"Shape of Y_test: \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "3Lz30rl196uR",
    "outputId": "7690c043-94cb-4fd6-8936-145a5a6e66ca"
   },
   "outputs": [],
   "source": [
    "img_vis(X_train,Y_train, 15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = (np.asfarray(X_train) / 255.0)\n",
    "X_test = (np.asfarray(X_test) / 255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Design a 5-layer Neural Network\n",
    "### 2.1 Design sub-layers and back-propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-uIKmCHnGy9"
   },
   "source": [
    "#### 2.1.1 Linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQ2S-RCDYznC"
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_size, output_size, learning_rate = 0.01):\n",
    "        self.info = \"Class for implementation of Linear Layer\"\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.W = np.random.randn(input_size, output_size)*0.01\n",
    "        self.b = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, S):\n",
    "        return np.dot(S, self.W) + self.b \n",
    "        \n",
    "    def backward(self, S, Z):\n",
    "        gradient = np.dot(Z, self.W.T)\n",
    "        delta_W = np.dot(S.T, Z)\n",
    "        delta_b  = Z.mean(axis=0)*S.shape[0]\n",
    "        self.W = self.W - self.learning_rate * delta_W  \n",
    "        self.b = self.b - self.learning_rate * delta_b\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 ReLU & LeakyReLU (LReLU) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.info = \"Class for implementation of ReLU Layer\"\n",
    "\n",
    "    def forward(self, S):\n",
    "        return np.maximum(0,S)\n",
    "\n",
    "    def backward(self, S, Z):\n",
    "        Z[S <= 0] = 0\n",
    "        return Z\n",
    "        \n",
    "class LReLU:\n",
    "    def __init__(self, hyperparameter = 0.01):\n",
    "        self.info = \"Class for implementation of Leaky ReLU Layer\"\n",
    "        self.hyperparameter = hyperparameter\n",
    "        \n",
    "    def forward(self, S):\n",
    "        return np.maximum(self.hyperparameter*S, S)\n",
    "    \n",
    "    def backward(self, S, Z):\n",
    "        Z[S <= self.hyperparameter*S] = 0\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 SoftMax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a) :\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 Cross-entropy loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_crossentropy(result,Y, derivate=False):\n",
    "    if derivate == False:\n",
    "        answers = result[np.arange(len(result)),Y]\n",
    "        cross_entropy = - answers + np.log(np.sum(np.exp(result),axis=-1))\n",
    "        return cross_entropy\n",
    "    else:\n",
    "        answers = np.zeros_like(result)\n",
    "        answers[np.arange(len(result)),Y] = 1\n",
    "        softmax = np.exp(result) / np.exp(result).sum(axis=-1,keepdims=True)\n",
    "        return (- answers + softmax) / result.shape[0]\n",
    "\n",
    "\n",
    "def grad_softmax_crossentropy(result,Y):\n",
    "    ones_for_answers = np.zeros_like(result)\n",
    "    ones_for_answers[np.arange(len(result)),Y] = 1    \n",
    "    softmax = np.exp(result) / np.exp(result).sum(axis=-1,keepdims=True)\n",
    "    return (- ones_for_answers + softmax) / result.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5 SGD\n",
    " Linear layer의 weights, bias update는 2.1.1 Linear class의 backward 함수에서 구현하였고, training data를 random한 batch로 나누는 과정은 Model class 의 train 함수 내부에 구현하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Design two 3-layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 \n",
    "Sequence: Input - Linear-ReLU - Linear-ReLU - Linear-SoftMax   \n",
    "The input and output size of NN: input 28x28, output 10   \n",
    "Layer Design: Linear(784,128) ㅡ> ReLU() ㅡ>  Linear(128,64) ㅡ> ReLU() ㅡ> Linear(64,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 \n",
    "Sequence: Input - Linear- LReLU - Linear- LReLU - Linear-SoftMax   \n",
    "The input and output size of NN: input 28x28, output 10    \n",
    "Layer Design: Linear(784,128) ㅡ> LReLU() ㅡ> Linear(128,64) ㅡ> LReLU() ㅡ> Linear(64,10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLaQNdOoKN4o"
   },
   "source": [
    "## 3. Design training process and train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "v9msXgF-JrLP",
    "outputId": "6568d6ba-8c35-413b-da57-bdf5d2545c80"
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, layers, num_epoch = 5000, batch_size = 512):\n",
    "        self.model = layers\n",
    "        self.epochs = num_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.train_loss_log = []\n",
    "        self.test_loss_log = []\n",
    "\n",
    "    def train(self, X_train, Y_train, X_test, Y_test):\n",
    "        model = self.model\n",
    "        for epoch in range(self.epochs):\n",
    "            for_minibatch = list(zip(X_train, Y_train))\n",
    "            random.shuffle(for_minibatch)\n",
    "            X, Y = zip(*for_minibatch)\n",
    "            X = list(X)\n",
    "            Y = list(Y)\n",
    "            batches = []\n",
    "            for i in range(0, len(Y), self.batch_size):\n",
    "                start = i\n",
    "                end = i+self.batch_size\n",
    "                batch = []\n",
    "                if end >= len(Y_train):\n",
    "                    end = len(Y_train)\n",
    "                batch.append(X[start:end])\n",
    "                batch.append(Y[start:end])\n",
    "                batches.append(batch)\n",
    "\n",
    "            for batch in batches:\n",
    "                X_batch = np.array(batch[0])\n",
    "                Y_batch = np.array(batch[1])\n",
    "                result_sqc = self.forward(X_batch)\n",
    "                layer_inputs = [X_batch]+result_sqc\n",
    "                result = result_sqc[-1]\n",
    "                loss = softmax_crossentropy(result,Y_batch)\n",
    "                loss_grad = grad_softmax_crossentropy(result,Y_batch)\n",
    "                # print(\"loss_grad\", loss_grad.shape)\n",
    "                # print(\"layer_inputs[layer_index]\",layer_inputs[-1].shape)\n",
    "                for layer_index in range(len(model))[::-1]:\n",
    "                    layer = model[layer_index]\n",
    "                    loss_grad = layer.backward(layer_inputs[layer_index],loss_grad)\n",
    "            #print(\"[after \"+str(epoch+1)+\"/\"+str(self.epochs)+\" epoch]\", \"train loss: \", self.calc_loss(X_train, Y_train),\"validation loss: \",  self.calc_loss(X_test, Y_test)  )\n",
    "            self.train_loss_log.append(self.calc_loss(X_train, Y_train))\n",
    "            self.test_loss_log.append(self.calc_loss(X_test, Y_test))\n",
    "                    \n",
    "    def forward(self, X):\n",
    "        result_sqc = []\n",
    "        input = X\n",
    "        for layer in self.model:\n",
    "            result_sqc.append(layer.forward(input))\n",
    "            input = result_sqc[-1]\n",
    "        return result_sqc\n",
    "\n",
    "    def predict(self, X):\n",
    "        # for prob, softmax layer!\n",
    "        result = self.forward(X)[-1]\n",
    "        prob = softmax(result)\n",
    "        predict_num = prob.argmax(axis=-1)\n",
    "        return predict_num, prob\n",
    "    \n",
    "    def calc_loss(self, X, Y):\n",
    "        layer_activations = self.forward(X)\n",
    "        terminal_weights = layer_activations[-1]\n",
    "        loss = softmax_crossentropy(terminal_weights,Y)\n",
    "        return np.mean(loss)\n",
    "\n",
    "    def calc_acc(self, X, Y):\n",
    "        forward_result =self.forward(X)[-1]\n",
    "        predict_list = forward_result.argmax(axis=-1)\n",
    "        acc = np.mean(predict_list==Y)\n",
    "        return acc\n",
    "\n",
    "    def loss_log_visualization(self):\n",
    "        plt.plot(self.train_loss_log, label='train loss')\n",
    "        plt.plot(self.test_loss_log, label='validation loss')\n",
    "        plt.xlabel(\"num iteration\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.legend(fontsize='x-large')\n",
    "    \n",
    "    def confusion_matrix_visualization(self, X_test, Y_test):\n",
    "        pred_list, prob_list = self.predict(X_test)\n",
    "        result = confusion_matrix(pred_list, Y_test.T, normalize=\"pred\")\n",
    "        sn.heatmap(result, annot=True, fmt='.2f', cmap=\"Blues\")\n",
    "        plt.xlabel(\"Predicted label\")\n",
    "        plt.ylabel(\"True label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layers = [Linear(784,128), ReLU(), Linear(128,64),ReLU(), Linear(64,10)]\n",
    "Model_ReLU = Model(layers)\n",
    "Model_ReLU.train(X_train, Y_train.T, X_test, Y_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [Linear(784,128), LReLU(), Linear(128,64), LReLU(), Linear(64,10)]\n",
    "Model_LReLU = Model(layers)\n",
    "Model_LReLU.train(X_train, Y_train.T, X_test, Y_test.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test the network on the test data and visualization the results on the report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Show 10x10 confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_ReLU.confusion_matrix_visualization(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_LReLU.confusion_matrix_visualization(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Show top 3 scored images with probability (for each class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_top3(model, X):\n",
    "    result = model.forward(X)[-1]\n",
    "    prob = softmax(result)\n",
    "    predict_num = prob.argmax(axis=-1)\n",
    "    return predict_num, prob\n",
    "\n",
    "def top3_scored_images_visualization(model, X_test, Y_test, desired_num):\n",
    "    class_desired_num = []\n",
    "    result_class, result_prob = predict_top3(model, X_test)\n",
    "    for i in range(len(Y_test)):\n",
    "        if result_class[i]==Y_test[i]: \n",
    "            if result_class[i]==desired_num:\n",
    "                idx = i\n",
    "                prob = result_prob[i][desired_num] * 100\n",
    "                temp = (prob, idx)\n",
    "                class_desired_num.append(temp)\n",
    "        else:\n",
    "            continue\n",
    "    class_desired_num.sort(key = lambda element : element[0])\n",
    "    \n",
    "    #print(class_desired_num)\n",
    "    fig, axes = plt.subplots(1, 3)\n",
    "    k = 1\n",
    "    for j in range(3):        \n",
    "        idx = class_desired_num[-1-j][1]\n",
    "        img  = X_test[idx].reshape(28,28)\n",
    "        prob = class_desired_num[-1-j][0]\n",
    "        plt.subplot(1,3,k)\n",
    "        plt.imshow(img, cmap=plt.cm.gray)\n",
    "        plt.title(str(prob)+\"%\")\n",
    "        k += 1\n",
    "    plt.suptitle('Top 3 Scored Image about '+ str(desired_num),fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_scored_images_visualization(Model_ReLU, X_test, Y_test, 0)\n",
    "top3_scored_images_visualization(Model_ReLU, X_test, Y_test, 1)\n",
    "top3_scored_images_visualization(Model_ReLU, X_test, Y_test, 2)\n",
    "top3_scored_images_visualization(Model_ReLU, X_test, Y_test, 3)\n",
    "top3_scored_images_visualization(Model_ReLU, X_test, Y_test, 4)\n",
    "top3_scored_images_visualization(Model_ReLU, X_test, Y_test, 5)\n",
    "top3_scored_images_visualization(Model_ReLU, X_test, Y_test, 6)\n",
    "top3_scored_images_visualization(Model_ReLU, X_test, Y_test, 7)\n",
    "top3_scored_images_visualization(Model_ReLU, X_test, Y_test, 8)\n",
    "top3_scored_images_visualization(Model_ReLU, X_test, Y_test, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_scored_images_visualization(Model_LReLU, X_test, Y_test, 0)\n",
    "top3_scored_images_visualization(Model_LReLU, X_test, Y_test, 1)\n",
    "top3_scored_images_visualization(Model_LReLU, X_test, Y_test, 2)\n",
    "top3_scored_images_visualization(Model_LReLU, X_test, Y_test, 3)\n",
    "top3_scored_images_visualization(Model_LReLU, X_test, Y_test, 4)\n",
    "top3_scored_images_visualization(Model_LReLU, X_test, Y_test, 5)\n",
    "top3_scored_images_visualization(Model_LReLU, X_test, Y_test, 6)\n",
    "top3_scored_images_visualization(Model_LReLU, X_test, Y_test, 7)\n",
    "top3_scored_images_visualization(Model_LReLU, X_test, Y_test, 8)\n",
    "top3_scored_images_visualization(Model_LReLU, X_test, Y_test, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Show training Loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_ReLU.loss_log_visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_LReLU.loss_log_visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model_ReLU accuracy: \", Model_ReLU.calc_acc(X_test,Y_test))\n",
    "print(\"Model_LReLU accuracy: \", Model_LReLU.calc_acc(X_test,Y_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2020DL-HW1-Shin.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
