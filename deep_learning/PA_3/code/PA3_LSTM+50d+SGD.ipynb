{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import emo_utils as utils\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import deque\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "def word_embedding(train_X):\n",
    "    new_train_X = []\n",
    "    for sentence in train_X:\n",
    "        temp = sentence.split()\n",
    "        blank = [x*0 for x in range(50)]\n",
    "        result = []\n",
    "        for word in temp:\n",
    "            word = word.lower()\n",
    "            result.append(word_to_vec_map[word])\n",
    "        while len(result) < 10:\n",
    "            result.append(blank)\n",
    "        new_train_X.append(result)\n",
    "    return new_train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = utils.read_csv(\"train_emoji.csv\")\n",
    "test_X, test_Y = utils.read_csv(\"test_emoji.csv\")\n",
    "words_to_index, index_to_words, word_to_vec_map = utils.read_glove_vecs(\"glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_train_X = word_embedding(train_X)\n",
    "embedded_test_X = word_embedding(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50)\n",
      "(132, 10, 50)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(embedded_train_X[0]).shape)\n",
    "print(np.array(embedded_train_X).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find Linear\n",
    "class Linear:\n",
    "    def __init__(self, input_size, output_size, learning_rate = 0.01):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.W = np.random.randn(input_size, output_size)*0.01\n",
    "        self.b = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, S):\n",
    "        self.input = S\n",
    "        self.output = np.dot(S, self.W) + self.b\n",
    "        return  self.output\n",
    "    \n",
    "    def backward(self, Z):\n",
    "        S = self.input\n",
    "        gradient = np.dot(Z, self.W.T)\n",
    "        delta_W = np.dot(S.T, Z)\n",
    "        delta_b  = Z.mean(axis=0)*S.shape[0]\n",
    "        self.W = self.W - self.learning_rate * delta_W  \n",
    "        self.b = self.b - self.learning_rate * delta_b\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(x):\n",
    "    return 1 - np.tanh(x) * np.tanh(x)\n",
    "\n",
    "def softmax(x, axis=None):\n",
    "    return np.exp(x - logsumexp(x, axis=axis, keepdims=True))\n",
    "\n",
    "\n",
    "def batch_softmax(input_array):\n",
    "    out = []\n",
    "    for row in input_array:\n",
    "        out.append(softmax(row, axis=1))\n",
    "    return np.stack(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNOptimizer(object):\n",
    "    def __init__(self,lr = 0.01,gradient_clipping = True):\n",
    "        self.lr = lr\n",
    "        self.gradient_clipping = gradient_clipping\n",
    "        self.first = True\n",
    "\n",
    "    def step(self):\n",
    "        for layer in self.model.layers:\n",
    "            for key in layer.params.keys():\n",
    "                if self.gradient_clipping:\n",
    "                    np.clip(layer.params[key]['deriv'], -2, 2, layer.params[key]['deriv'])\n",
    "\n",
    "                self._update_rule(param=layer.params[key]['value'], grad=layer.params[key]['deriv'])\n",
    "\n",
    "    def _update_rule(self, **kwargs):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(RNNOptimizer):\n",
    "    def __init__(self,\n",
    "                 lr= 0.01,\n",
    "                 gradient_clipping = True):\n",
    "        super().__init__(lr, gradient_clipping)\n",
    "\n",
    "    def _update_rule(self, **kwargs):\n",
    "\n",
    "        update = self.lr*kwargs['grad']\n",
    "        kwargs['param'] -= update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropy:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, result, Y):\n",
    "        answers = result[np.arange(len(result)),Y]\n",
    "        cross_entropy = - answers + np.log(np.sum(np.exp(result),axis=-1))\n",
    "        return cross_entropy\n",
    "\n",
    "    def backward(self, result, Y):\n",
    "        ones_for_answers = np.zeros_like(result)\n",
    "        ones_for_answers[np.arange(len(result)),Y] = 1    \n",
    "        softmax = np.exp(result) / np.exp(result).sum(axis=-1,keepdims=True)\n",
    "        return (- ones_for_answers + softmax) / result.shape[0]\n",
    "\n",
    "    def _output(self):\n",
    "        self.softmax_preds = np.clip(self.prediction, self.eps, 1 - self.eps)\n",
    "        softmax_cross_entropy_loss = -1.0 * self.target * np.log(self.softmax_preds) - (1.0 - self.target) * np.log(1 - self.softmax_preds)\n",
    "\n",
    "        return np.sum(softmax_cross_entropy_loss)\n",
    "\n",
    "    def _input_grad(self):\n",
    "        return self.softmax_preds - self.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNode:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, X_in, H_in, C_in, params_dict):\n",
    "        self.X_in = X_in\n",
    "        self.C_in = C_in\n",
    "\n",
    "        self.Z = np.column_stack((X_in, H_in))\n",
    "        \n",
    "        self.f_int = np.dot(self.Z, params_dict['W_f']['value']) + params_dict['B_f']['value']\n",
    "        self.f = sigmoid(self.f_int)\n",
    "        \n",
    "        self.i_int = np.dot(self.Z, params_dict['W_i']['value']) + params_dict['B_i']['value']\n",
    "        self.i = sigmoid(self.i_int)\n",
    "        self.C_bar_int = np.dot(self.Z, params_dict['W_c']['value']) + params_dict['B_c']['value']\n",
    "        self.C_bar = tanh(self.C_bar_int)\n",
    "\n",
    "        self.C_out = self.f * C_in + self.i * self.C_bar\n",
    "        self.o_int = np.dot(self.Z, params_dict['W_o']['value']) + params_dict['B_o']['value']\n",
    "        self.o = sigmoid(self.o_int)\n",
    "        self.H_out = self.o * tanh(self.C_out)\n",
    "\n",
    "        self.X_out = np.dot(self.H_out, params_dict['W_v']['value']) + params_dict['B_v']['value']\n",
    "        \n",
    "        return self.X_out, self.H_out, self.C_out \n",
    "\n",
    "\n",
    "    def backward(self, X_out_grad, H_out_grad, C_out_grad, params_dict):\n",
    "        params_dict['W_v']['deriv'] += np.dot(self.H_out.T, X_out_grad)\n",
    "        params_dict['B_v']['deriv'] += X_out_grad.sum(axis=0)\n",
    "\n",
    "        dh_out = np.dot(X_out_grad, params_dict['W_v']['value'].T)        \n",
    "        dh_out += H_out_grad\n",
    "                         \n",
    "        do = dh_out * tanh(self.C_out)\n",
    "        do_int = dsigmoid(self.o_int) * do\n",
    "        params_dict['W_o']['deriv'] += np.dot(self.Z.T, do_int)\n",
    "        params_dict['B_o']['deriv'] += do_int.sum(axis=0)\n",
    "\n",
    "        dC_out = dh_out * self.o * dtanh(self.C_out)\n",
    "        dC_out += C_out_grad\n",
    "        dC_bar = dC_out * self.i\n",
    "        dC_bar_int = dtanh(self.C_bar_int) * dC_bar\n",
    "        params_dict['W_c']['deriv'] += np.dot(self.Z.T, dC_bar_int)\n",
    "        params_dict['B_c']['deriv'] += dC_bar_int.sum(axis=0)\n",
    "\n",
    "        di = dC_out * self.C_bar\n",
    "        di_int = dsigmoid(self.i_int) * di\n",
    "        params_dict['W_i']['deriv'] += np.dot(self.Z.T, di_int)\n",
    "        params_dict['B_i']['deriv'] += di_int.sum(axis=0)\n",
    "\n",
    "        df = dC_out * self.C_in\n",
    "        df_int = dsigmoid(self.f_int) * df\n",
    "        params_dict['W_f']['deriv'] += np.dot(self.Z.T, df_int)\n",
    "        params_dict['B_f']['deriv'] += df_int.sum(axis=0)\n",
    "\n",
    "        dz = (np.dot(df_int, params_dict['W_f']['value'].T)\n",
    "             + np.dot(di_int, params_dict['W_i']['value'].T)\n",
    "             + np.dot(dC_bar_int, params_dict['W_c']['value'].T)\n",
    "             + np.dot(do_int, params_dict['W_o']['value'].T))\n",
    "    \n",
    "        dx_prev = dz[:, :self.X_in.shape[1]]\n",
    "        dH_prev = dz[:, self.X_in.shape[1]:]\n",
    "        dC_prev = self.f * dC_out\n",
    "\n",
    "        return dx_prev, dH_prev, dC_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer:\n",
    "\n",
    "    def __init__(self,hidden_size,output_size,weight_scale = 0.01):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weight_scale = weight_scale\n",
    "        self.start_H = np.zeros((1, hidden_size))\n",
    "        self.start_C = np.zeros((1, hidden_size))        \n",
    "        self.first = True\n",
    "        self.first_backward= True\n",
    "\n",
    "        \n",
    "    def _init_params(self,\n",
    "                     input_):\n",
    "        \n",
    "        self.vocab_size = input_.shape[2]\n",
    "\n",
    "        self.params = {}\n",
    "        self.params['W_f'] = {}\n",
    "        self.params['B_f'] = {}\n",
    "        self.params['W_i'] = {}\n",
    "        self.params['B_i'] = {}\n",
    "        self.params['W_c'] = {}\n",
    "        self.params['B_c'] = {}\n",
    "        self.params['W_o'] = {}\n",
    "        self.params['B_o'] = {}        \n",
    "        self.params['W_v'] = {}\n",
    "        self.params['B_v'] = {}\n",
    "        \n",
    "        self.params['W_f']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size =(self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_f']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size=(1, self.hidden_size))\n",
    "        self.params['W_i']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size=(self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_i']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size=(1, self.hidden_size))\n",
    "        self.params['W_c']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size=(self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_c']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size=(1, self.hidden_size))\n",
    "        self.params['W_o']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size=(self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_o']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size=(1, self.hidden_size))       \n",
    "        self.params['W_v']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size=(self.hidden_size, self.output_size))\n",
    "        self.params['B_v']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size=(1, self.output_size))\n",
    "        \n",
    "        #print(\"--------------------------------------------------------------------\")\n",
    "        #print(\"input_.shape\", input_.shape)\n",
    "        for key in self.params.keys():\n",
    "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['value'])\n",
    "        self.cells = [LSTMNode() for x in range(input_.shape[1])]\n",
    "\n",
    "\n",
    "    def _clear_gradients(self):\n",
    "        for key in self.params.keys():\n",
    "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['deriv'])\n",
    "                    \n",
    "        \n",
    "    def forward(self, x_seq_in):\n",
    "        self.first_backward= True\n",
    "        if self.first:\n",
    "            self._init_params(x_seq_in)\n",
    "            self.first=False\n",
    "        \n",
    "        batch_size = x_seq_in.shape[0]\n",
    "        \n",
    "        H_in = np.copy(self.start_H)\n",
    "        C_in = np.copy(self.start_C)\n",
    "        \n",
    "        H_in = np.repeat(H_in, batch_size, axis=0)\n",
    "        C_in = np.repeat(C_in, batch_size, axis=0)        \n",
    "\n",
    "        sequence_length = x_seq_in.shape[1]\n",
    "        \n",
    "        x_seq_out = np.zeros((batch_size, sequence_length, self.output_size))\n",
    "        \n",
    "        for t in range(sequence_length):\n",
    "            x_in = x_seq_in[:, t, :]\n",
    "            y_out, H_in, C_in = self.cells[t].forward(x_in, H_in, C_in, self.params)\n",
    "            x_seq_out[:, t, :] = y_out\n",
    "    \n",
    "        self.start_H = H_in.mean(axis=0, keepdims=True)\n",
    "        self.start_C = C_in.mean(axis=0, keepdims=True)        \n",
    "        \n",
    "        return x_seq_out, y_out\n",
    "\n",
    "\n",
    "    def backward(self, x_seq_out_grad):\n",
    "        batch_size = x_seq_out_grad.shape[0]\n",
    "        \n",
    "        h_in_grad = np.zeros((batch_size, self.hidden_size))\n",
    "        c_in_grad = np.zeros((batch_size, self.hidden_size))        \n",
    "        \n",
    "        num_chars = x_seq_out_grad.shape[1]\n",
    "        \n",
    "        x_seq_in_grad = np.zeros((batch_size, num_chars, self.vocab_size))\n",
    "        \n",
    "        for t in reversed(range(10)):\n",
    "            x_out_grad = x_seq_out_grad\n",
    "            grad_out, h_in_grad, c_in_grad = self.cells[t].backward(x_out_grad, h_in_grad, c_in_grad, self.params)\n",
    "            x_seq_in_grad[:, t, :] = grad_out\n",
    "            \n",
    "        return x_seq_in_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, layers, sequence_length, vocab_size, hidden_size,loss):\n",
    "        self.layers = layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.loss = loss\n",
    "        self.Linear = Linear(vocab_size, 5)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            setattr(layer, 'sequence_length', sequence_length)\n",
    "\n",
    "        \n",
    "    def forward(self, x_batch):  \n",
    "        for layer in self.layers:\n",
    "            total, x_batch = layer.forward(x_batch)\n",
    "        #print(\"total\", total.shape)\n",
    "        #print(\"x_batch after LSTM\", x_batch.shape)\n",
    "        \n",
    "        x_batch = self.Linear.forward(x_batch)\n",
    "        #print(\"x_batch atfer Linear\", x_batch.shape)\n",
    "        return x_batch\n",
    "        \n",
    "    def backward(self, loss_grad):\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_grad = layer.backward(loss_grad)\n",
    "            \n",
    "        return loss_grad\n",
    "                \n",
    "    def single_step(self, x_batch, y_batch):\n",
    "\n",
    "        x_batch_out = self.forward(x_batch)\n",
    "        #print(\"x_batch_out\", x_batch_out.shape)\n",
    "        predict, _ = self.predict(x_batch_out, y_batch)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            layer._clear_gradients()\n",
    "            \n",
    "        loss = self.loss.forward(x_batch_out, y_batch)\n",
    "        loss_grad = self.loss.backward(x_batch_out, y_batch)\n",
    "        loss_grad = self.Linear.backward(loss_grad)\n",
    "        self.backward(loss_grad)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def predict(self, X, Y):\n",
    "        pred = []\n",
    "        for probs in X:\n",
    "            pred.append(np.argmax(probs))\n",
    "        acc = 0\n",
    "        for i in range(len(Y)):\n",
    "            if pred[i] == Y[i]:\n",
    "                acc += 1\n",
    "        acc = acc/len(Y) * 100\n",
    "        return pred, acc\n",
    "    \n",
    "    def evaluate(self, x_batch, y_batch):\n",
    "        x_batch_out = self.forward(x_batch)\n",
    "        loss = self.loss.forward(x_batch_out, y_batch)\n",
    "        return loss\n",
    "    \n",
    "    def make_result(self, X, Y):\n",
    "        X_out = self.forward(X)\n",
    "        pred = []\n",
    "        for x in X_out:\n",
    "            pred.append(np.argmax(x))\n",
    "        acc = 0\n",
    "        for i in range(len(Y)):\n",
    "            if pred[i] == Y[i]:\n",
    "                acc += 1\n",
    "        acc = acc/len(Y) * 100\n",
    "        return pred, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTrainer:\n",
    "    def __init__(self, train_X, train_Y, test_X, test_Y, model,optim):\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y\n",
    "        self.model = model\n",
    "        self.sequence_length = self.model.sequence_length\n",
    "        self.optim = optim\n",
    "        self.train_loss_log = []\n",
    "        self.test_loss_log = []\n",
    "        self.log_step = []\n",
    "        setattr(self.optim, 'model', self.model)\n",
    "\n",
    "    def visualize_loss(self):\n",
    "        plt.plot(self.log_step, self.train_loss_log, label='train loss')\n",
    "        plt.plot(self.log_step, self.test_loss_log, label='validation loss')\n",
    "        plt.xlabel(\"num iteration\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.legend(fontsize='x-large')\n",
    "        \n",
    "    def train(self, num_iterations):\n",
    "        train_X = self.train_X\n",
    "        train_Y = self.train_Y\n",
    "        test_X = self.test_X\n",
    "        test_Y = self.test_Y \n",
    "        \n",
    "        num_iter = 0\n",
    "        \n",
    "        train_moving_average = deque(maxlen=100)\n",
    "        test_moving_average = deque(maxlen=100)\n",
    "        \n",
    "        while num_iter < num_iterations:\n",
    "            inputs_batch = np.array(train_X)\n",
    "            targets_batch = np.array(train_Y)\n",
    "            train_loss = self.model.single_step(inputs_batch, targets_batch)\n",
    "            test_loss = self.model.evaluate(np.array(test_X), np.array(test_Y))\n",
    "            train_moving_average.append(train_loss)\n",
    "            train_ma_loss = np.mean(train_moving_average)            \n",
    "            test_moving_average.append(test_loss)\n",
    "            test_ma_loss = np.mean(test_moving_average)\n",
    "            self.train_loss_log.append(train_ma_loss)\n",
    "            self.test_loss_log.append(test_ma_loss)\n",
    "            self.optim.step()\n",
    "            self.log_step.append(num_iter)\n",
    "            if num_iter % 100 == 0:\n",
    "                print(\"[Loss after %d iter] train loss: %f, test loss: %f\" % (num_iter, train_ma_loss, test_ma_loss))\n",
    "            num_iter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loss after 0 iter] train loss: 1.609466, test loss: 1.609214\n",
      "[Loss after 100 iter] train loss: 1.598585, test loss: 1.595450\n",
      "[Loss after 200 iter] train loss: 1.581612, test loss: 1.573264\n",
      "[Loss after 300 iter] train loss: 1.569529, test loss: 1.556423\n",
      "[Loss after 400 iter] train loss: 1.561402, test loss: 1.544003\n",
      "[Loss after 500 iter] train loss: 1.556832, test loss: 1.536003\n",
      "[Loss after 600 iter] train loss: 1.554776, test loss: 1.531682\n",
      "[Loss after 700 iter] train loss: 1.553957, test loss: 1.529649\n",
      "[Loss after 800 iter] train loss: 1.553571, test loss: 1.528757\n",
      "[Loss after 900 iter] train loss: 1.553277, test loss: 1.528359\n",
      "[Loss after 1000 iter] train loss: 1.552942, test loss: 1.528143\n",
      "[Loss after 1100 iter] train loss: 1.552483, test loss: 1.527947\n",
      "[Loss after 1200 iter] train loss: 1.551786, test loss: 1.527650\n",
      "[Loss after 1300 iter] train loss: 1.550586, test loss: 1.527050\n",
      "[Loss after 1400 iter] train loss: 1.547976, test loss: 1.525476\n",
      "[Loss after 1500 iter] train loss: 1.537647, test loss: 1.518296\n",
      "[Loss after 1600 iter] train loss: 1.400045, test loss: 1.430262\n",
      "[Loss after 1700 iter] train loss: 1.273939, test loss: 1.357545\n",
      "[Loss after 1800 iter] train loss: 1.158349, test loss: 1.257006\n",
      "[Loss after 1900 iter] train loss: 1.096745, test loss: 1.217863\n",
      "[Loss after 2000 iter] train loss: 1.057215, test loss: 1.219896\n",
      "[Loss after 2100 iter] train loss: 1.020132, test loss: 1.240899\n",
      "[Loss after 2200 iter] train loss: 0.991365, test loss: 1.287664\n",
      "[Loss after 2300 iter] train loss: 0.995623, test loss: 1.337088\n",
      "[Loss after 2400 iter] train loss: 1.000317, test loss: 1.411550\n",
      "[Loss after 2500 iter] train loss: 0.979261, test loss: 1.427480\n",
      "[Loss after 2600 iter] train loss: 0.891555, test loss: 1.413995\n",
      "[Loss after 2700 iter] train loss: 0.845906, test loss: 1.429960\n",
      "[Loss after 2800 iter] train loss: 0.770594, test loss: 1.418512\n",
      "[Loss after 2900 iter] train loss: 0.662681, test loss: 1.343791\n",
      "[Loss after 3000 iter] train loss: 0.559551, test loss: 1.367456\n"
     ]
    }
   ],
   "source": [
    "layers = [LSTMLayer(hidden_size=256, output_size=50, weight_scale=0.01)]\n",
    "model = Model(layers=layers, vocab_size=50, hidden_size=256, sequence_length=10,loss=SoftmaxCrossEntropy())\n",
    "optimizer = SGD(lr=0.02, gradient_clipping=True)\n",
    "RNN = RNNTrainer(embedded_train_X, train_Y, embedded_test_X, test_Y, model, optimizer)\n",
    "RNN.train(3001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_to_emoji(pred):\n",
    "    for i in pred:\n",
    "        print(utils.label_to_emoji(i), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN model accuracy for test set is 71.43%\n"
     ]
    }
   ],
   "source": [
    "pred, acc = RNN.model.make_result(np.array(embedded_test_X), test_Y)\n",
    "print(\"RNN model accuracy for test set is \"+ str(round(acc,2))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ´ ðŸ˜ž ðŸ˜„ ðŸ˜„ ðŸ˜„ ðŸ˜„ âš¾ ðŸ˜„ ðŸ´ ðŸ˜„ âš¾ â¤ï¸ â¤ï¸ ðŸ˜ž âš¾ ðŸ˜ž â¤ï¸ ðŸ˜„ âš¾ ðŸ˜„ ðŸ˜ž â¤ï¸ ðŸ´ ðŸ˜ž ðŸ˜ž âš¾ âš¾ â¤ï¸ âš¾ ðŸ˜„ â¤ï¸ âš¾ ðŸ˜ž ðŸ˜„ ðŸ˜„ ðŸ˜ž ðŸ˜„ ðŸ´ ðŸ´ ðŸ˜„ âš¾ â¤ï¸ â¤ï¸ âš¾ ðŸ˜„ ðŸ˜ž â¤ï¸ ðŸ˜„ ðŸ˜ž âš¾ âš¾ ðŸ˜ž ðŸ˜ž ðŸ˜„ ðŸ˜„ ðŸ˜„ "
     ]
    }
   ],
   "source": [
    "pred_to_emoji(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "RNN.visualize_loss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
