{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import emo_utils as utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def word_embedding(train_X):\n",
    "    new_train_X = []\n",
    "    for sentence in train_X:\n",
    "        temp = sentence.split()\n",
    "        result = []\n",
    "        for word in temp:\n",
    "            word = word.lower()\n",
    "            result.append(word_to_vec_map[word])\n",
    "        new_train_X.append(result)\n",
    "    return new_train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = utils.read_csv(\"train_emoji.csv\")\n",
    "test_X, test_Y = utils.read_csv(\"test_emoji.csv\")\n",
    "words_to_index, index_to_words, word_to_vec_map = utils.read_glove_vecs(\"glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_train_X = word_embedding(train_X)\n",
    "embedded_test_X = word_embedding(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "never talk to me again\n",
      "132\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "print(train_X[0])\n",
    "print(len(train_Y))\n",
    "print(len(test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def forward(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def backward(self, x, top_diff):\n",
    "        output = self.forward(x)\n",
    "        return (1.0 - output) * output * top_diff\n",
    "\n",
    "class Tanh:\n",
    "    def forward(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def backward(self, x, top_diff):\n",
    "        output = self.forward(x)\n",
    "        return (1.0 - np.square(output)) * top_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplyGate:\n",
    "    def forward(self,W, x):\n",
    "        return np.dot(W, x)\n",
    "\n",
    "    def backward(self, W, x, dz):\n",
    "        dW = np.asarray(np.dot(np.transpose(np.asmatrix(dz)), np.asmatrix(x)))\n",
    "        dx = np.dot(np.transpose(W), dz)\n",
    "        return dW, dx\n",
    "\n",
    "class AddGate:\n",
    "    def forward(self, x1, x2):\n",
    "        return x1 + x2\n",
    "\n",
    "    def backward(self, x1, x2, dz):\n",
    "        dx1 = dz * np.ones_like(x1)\n",
    "        dx2 = dz * np.ones_like(x2)\n",
    "        return dx1, dx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mulGate = MultiplyGate()\n",
    "addGate = AddGate()\n",
    "activation = Tanh()\n",
    "\n",
    "class RNNLayer:\n",
    "    def forward(self, x, prev_s, U, W, V):\n",
    "        self.mulu = mulGate.forward(U, x)\n",
    "        self.mulw = mulGate.forward(W, prev_s)\n",
    "        self.add = addGate.forward(self.mulw, self.mulu)\n",
    "        self.s = activation.forward(self.add)\n",
    "        self.mulv = mulGate.forward(V, self.s)\n",
    "        #print(\"self.s.shape\", self.s.shape)\n",
    "        #print(\"RNN forward result: \", self.mulv.shape)\n",
    "\n",
    "    def backward(self, x, prev_s, U, W, V, diff_s, dmulv):\n",
    "        self.forward(x, prev_s, U, W, V)\n",
    "        dV, dsv = mulGate.backward(V, self.s, dmulv)\n",
    "        ds = dsv + diff_s\n",
    "        dadd = activation.backward(self.add, ds)\n",
    "        dmulw, dmulu = addGate.backward(self.mulw, self.mulu, dadd)\n",
    "        dW, dprev_s = mulGate.backward(W, prev_s, dmulw)\n",
    "        dU, dx = mulGate.backward(U, x, dmulu)\n",
    "        return (dprev_s, dU, dW, dV)\n",
    "    \n",
    "class Linear:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.info = \"Linear Layer\"\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.1\n",
    "        self.biases = np.zeros(output_size)\n",
    "        self.learning_rate = 0.5\n",
    "        self.last_input_shape = None\n",
    "        self.last_input = None\n",
    "        self.last_output = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.last_input_shape = input.shape\n",
    "        input = input.flatten()\n",
    "        output = np.dot(input, self.weights) + self.biases\n",
    "\n",
    "        self.last_input = input\n",
    "        self.last_output = output\n",
    "        return np.exp(output) / np.sum(np.exp(output), axis=0)\n",
    "\n",
    "    def backward(self, din):\n",
    "        for i, gradient in enumerate(din):\n",
    "            if gradient == 0:\n",
    "                continue\n",
    "\n",
    "            t_exp = np.exp(self.last_output)\n",
    "            dout_dt = -t_exp[i] * t_exp / (np.sum(t_exp) ** 2)\n",
    "            dout_dt[i] = t_exp[i] * (np.sum(t_exp) - t_exp[i]) / (np.sum(t_exp) ** 2)\n",
    "\n",
    "            dt = gradient * dout_dt\n",
    "\n",
    "            dout = self.weights @ dt\n",
    "\n",
    "            self.weights -= self.learning_rate * (np.transpose(self.last_input[np.newaxis]) @ dt[np.newaxis])\n",
    "            self.biases -= self.learning_rate * dt\n",
    "\n",
    "            return dout.reshape(self.last_input_shape)\n",
    "\n",
    "    def get_weights(self):\n",
    "        return np.reshape(self.weights, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def predict(self, x):\n",
    "        exp_scores = np.exp(x)\n",
    "        return exp_scores / np.sum(exp_scores)\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        probs = self.predict(x)\n",
    "        return -np.log(probs[y])\n",
    "\n",
    "    def diff(self, x, y):\n",
    "        probs = self.predict(x)\n",
    "        #probs[y] -= 1.0\n",
    "        probs = x - y\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98884662 0.31707163 0.43547845]\n",
      " [0.13334591 0.37768065 0.04957284]]\n",
      "[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n",
      " -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n",
      " -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n",
      " -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n",
      "  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n",
      "  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n",
      "  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n",
      " -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n",
      " -2.6671e-01  9.2121e-01]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.uniform(0, 1, (2,3))\n",
    "print(a)\n",
    "print(embedded_test_X[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        self.U = np.random.uniform(-np.sqrt(1. / word_dim), np.sqrt(1. / word_dim), (hidden_dim, word_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (hidden_dim, hidden_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (word_dim, hidden_dim))\n",
    "        self.train_loss_log = []\n",
    "        self.test_loss_log = []\n",
    "        self.losses = []\n",
    "        self.log_step = []\n",
    "        self.Linear = Linear(word_dim, 5)\n",
    "        \n",
    "    def visualize_loss(self):\n",
    "        plt.plot(self.log_step, self.train_loss_log, label='train loss')\n",
    "        plt.plot(self.log_step, self.test_loss_log, label='validation loss')\n",
    "        plt.xlabel(\"num iteration\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.legend(fontsize='x-large')\n",
    "        pass\n",
    "    \n",
    "    def cross_entropy(self, x):\n",
    "        return -np.log(x)\n",
    "    \n",
    "    # find forward_propagation\n",
    "    def forward_propagation(self, x):\n",
    "        # The total number of time steps\n",
    "        # x = [[50개] [50개] [50개] [50개] [50개]]\n",
    "        T = len(x) # 5\n",
    "        layers = []\n",
    "        prev_s = np.zeros(self.hidden_dim)\n",
    "        # For each time step...\n",
    "        for t in range(T):\n",
    "            # len(x[t]) => 50\n",
    "            layer = RNNLayer()\n",
    "            input = x[t]\n",
    "            layer.forward(input, prev_s, self.U, self.W, self.V)\n",
    "            layer.input = x[t]\n",
    "            prev_s = layer.s\n",
    "            layers.append(layer)\n",
    "        output = layer.mulv\n",
    "        result = self.Linear.forward(output)\n",
    "        return result, layers\n",
    "\n",
    "    def predict(self, X, Y):\n",
    "        pred = []\n",
    "        for x in X:\n",
    "            result, layers = self.forward_propagation(x)\n",
    "            pred.append(np.argmax(result))\n",
    "        acc = 0\n",
    "        for i in range(len(Y)):\n",
    "            if pred[i] == Y[i]:\n",
    "                acc += 1\n",
    "        acc = acc/len(Y) * 100\n",
    "        return pred, acc\n",
    "\n",
    "    def calculate_loss(self, x, y):\n",
    "        result, layers = self.forward_propagation(x)\n",
    "        #print(\"result\", result)\n",
    "        #print(\"result[y]\", result[y])\n",
    "        return self.cross_entropy(result[y])\n",
    "\n",
    "    def calculate_total_loss(self, X, Y):\n",
    "        loss = 0.0\n",
    "        for i in range(len(Y)):\n",
    "            loss += self.calculate_loss(X[i], Y[i])\n",
    "        return loss / float(len(Y))\n",
    "\n",
    "    # find backpropagation\n",
    "    def back_propagation(self, x, y):\n",
    "        result, layers = self.forward_propagation(x)\n",
    "        gradient = np.zeros(5)\n",
    "        gradient[y] = -1 / result[y]\n",
    "        gradient = self.Linear.backward(gradient)\n",
    "        \n",
    "        # --> Linear Layer에서 back propagation 함\n",
    "        output = Softmax()\n",
    "        dU = np.zeros(self.U.shape)\n",
    "        dV = np.zeros(self.V.shape)\n",
    "        dW = np.zeros(self.W.shape)\n",
    "\n",
    "        T = len(layers) - 1\n",
    "        prev_s_t = np.zeros(self.hidden_dim)\n",
    "        diff_s = np.zeros(self.hidden_dim)\n",
    "        \n",
    "        for t in range(0, T):\n",
    "            dmulv = output.diff(layers[t].mulv, gradient)\n",
    "            input = np.zeros(self.word_dim)\n",
    "            dprev_s, dU_t, dW_t, dV_t = layers[t].backward(input, prev_s_t, self.U, self.W, self.V, diff_s, dmulv)\n",
    "            prev_s_t = layers[t].s\n",
    "            dmulv = np.zeros(self.word_dim)\n",
    "            for i in range(t-1, max(-1, t-self.bptt_truncate-1), -1):\n",
    "                input = np.zeros(self.word_dim)\n",
    "                #input[x[i]] = 1\n",
    "                input = layers[i].input\n",
    "                prev_s_i = np.zeros(self.hidden_dim) if i == 0 else layers[i-1].s\n",
    "                dprev_s, dU_i, dW_i, dV_i = layers[i].backward(input, prev_s_i, self.U, self.W, self.V, dprev_s, dmulv)\n",
    "                dU_t += dU_i\n",
    "                dW_t += dW_i\n",
    "            dV += dV_t\n",
    "            dU += dU_t\n",
    "            dW += dW_t\n",
    "        return (dU, dW, dV)\n",
    "\n",
    "    def sgd_step(self, x, y, learning_rate):\n",
    "        dU, dW, dV = self.back_propagation(x, y)\n",
    "        self.U -= learning_rate * dU\n",
    "        self.V -= learning_rate * dV\n",
    "        self.W -= learning_rate * dW\n",
    "\n",
    "    def train(self, X, Y, test_X, test_Y, learning_rate=0.005, nepoch=100):\n",
    "        \"\"\"\n",
    "        X = [[never talk to me again] ~ [i love you]]\n",
    "        embedded X = [[[0.1, 0.2, ... , 0.1 ], [0.1, 0.2, ... , 0.1 ], [0.1, 0.2, ... , 0.1 ]]         ~ [[] [] []]             ] \n",
    "        \"\"\"\n",
    "        num_examples_seen = 0\n",
    "        \n",
    "        for epoch in range(nepoch):\n",
    "            if epoch % 10 == 0:\n",
    "                # 여기서 forward\n",
    "                train_loss = self.calculate_total_loss(X, Y)\n",
    "                self.losses.append((num_examples_seen, train_loss))\n",
    "                self.train_loss_log.append(train_loss)\n",
    "                test_loss = self.calculate_total_loss(test_X, test_Y)\n",
    "                self.test_loss_log.append(test_loss)\n",
    "                self.log_step.append(epoch)\n",
    "                #print(\"loss\", loss)\n",
    "                print(\"[Loss after epoch=%d] train loss: %f, test loss: %f\" % (epoch, train_loss, test_loss))\n",
    "                \n",
    "            # For each training example...\n",
    "            for i in range(len(Y)):\n",
    "                self.sgd_step(X[i], Y[i], learning_rate)\n",
    "                num_examples_seen += 1\n",
    "        print(\"training done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loss after epoch=0] train loss: 1.631724, test loss: 1.625062\n",
      "[Loss after epoch=10] train loss: 0.978340, test loss: 1.035046\n",
      "[Loss after epoch=20] train loss: 1.087604, test loss: 1.188837\n",
      "[Loss after epoch=30] train loss: 0.925598, test loss: 0.979347\n",
      "[Loss after epoch=40] train loss: 1.164572, test loss: 1.419573\n",
      "[Loss after epoch=50] train loss: 1.028679, test loss: 1.243604\n",
      "[Loss after epoch=60] train loss: 1.068026, test loss: 1.263582\n",
      "[Loss after epoch=70] train loss: 0.724529, test loss: 0.825077\n",
      "[Loss after epoch=80] train loss: 0.965887, test loss: 1.066392\n",
      "[Loss after epoch=90] train loss: 0.795123, test loss: 0.893953\n",
      "[Loss after epoch=100] train loss: 0.723644, test loss: 0.808951\n",
      "[Loss after epoch=110] train loss: 0.713910, test loss: 0.761840\n",
      "[Loss after epoch=120] train loss: 0.890704, test loss: 1.031198\n",
      "[Loss after epoch=130] train loss: 0.769440, test loss: 0.792611\n",
      "[Loss after epoch=140] train loss: 0.744104, test loss: 0.968533\n",
      "[Loss after epoch=150] train loss: 0.742843, test loss: 0.954708\n",
      "[Loss after epoch=160] train loss: 0.902301, test loss: 1.114991\n",
      "[Loss after epoch=170] train loss: 0.921234, test loss: 1.019494\n",
      "[Loss after epoch=180] train loss: 0.960104, test loss: 1.144963\n",
      "[Loss after epoch=190] train loss: 0.873569, test loss: 1.052471\n",
      "[Loss after epoch=200] train loss: 0.871822, test loss: 1.027101\n",
      "[Loss after epoch=210] train loss: 0.842443, test loss: 1.051060\n",
      "[Loss after epoch=220] train loss: 0.804750, test loss: 1.016657\n",
      "[Loss after epoch=230] train loss: 0.808174, test loss: 1.014263\n",
      "[Loss after epoch=240] train loss: 0.785619, test loss: 0.984970\n",
      "[Loss after epoch=250] train loss: 0.744121, test loss: 0.916672\n",
      "[Loss after epoch=260] train loss: 0.724503, test loss: 0.876734\n",
      "[Loss after epoch=270] train loss: 0.719463, test loss: 0.868661\n",
      "[Loss after epoch=280] train loss: 0.701398, test loss: 0.855799\n",
      "[Loss after epoch=290] train loss: 0.688605, test loss: 0.852359\n",
      "[Loss after epoch=300] train loss: 0.675391, test loss: 0.855856\n",
      "[Loss after epoch=310] train loss: 0.674924, test loss: 0.857349\n",
      "[Loss after epoch=320] train loss: 0.655718, test loss: 0.799619\n",
      "[Loss after epoch=330] train loss: 0.644417, test loss: 0.718618\n",
      "[Loss after epoch=340] train loss: 0.661197, test loss: 0.690308\n",
      "[Loss after epoch=350] train loss: 0.613107, test loss: 0.673564\n",
      "[Loss after epoch=360] train loss: 0.622010, test loss: 0.708093\n",
      "[Loss after epoch=370] train loss: 0.626423, test loss: 0.728740\n",
      "[Loss after epoch=380] train loss: 0.626222, test loss: 0.751399\n",
      "[Loss after epoch=390] train loss: 0.628797, test loss: 0.774962\n",
      "[Loss after epoch=400] train loss: 0.611258, test loss: 0.778810\n",
      "[Loss after epoch=410] train loss: 0.596680, test loss: 0.779027\n",
      "[Loss after epoch=420] train loss: 0.588852, test loss: 0.778862\n",
      "[Loss after epoch=430] train loss: 0.591051, test loss: 0.783310\n",
      "[Loss after epoch=440] train loss: 0.588679, test loss: 0.783502\n",
      "[Loss after epoch=450] train loss: 0.588985, test loss: 0.784409\n",
      "[Loss after epoch=460] train loss: 0.583907, test loss: 0.780379\n",
      "[Loss after epoch=470] train loss: 0.579125, test loss: 0.775824\n",
      "[Loss after epoch=480] train loss: 0.575634, test loss: 0.772647\n",
      "[Loss after epoch=490] train loss: 0.573324, test loss: 0.771028\n",
      "[Loss after epoch=500] train loss: 0.572021, test loss: 0.770898\n",
      "[Loss after epoch=510] train loss: 0.571577, test loss: 0.772098\n",
      "[Loss after epoch=520] train loss: 0.571905, test loss: 0.774450\n",
      "[Loss after epoch=530] train loss: 0.568177, test loss: 0.773025\n",
      "[Loss after epoch=540] train loss: 0.564243, test loss: 0.770481\n",
      "[Loss after epoch=550] train loss: 0.561044, test loss: 0.768425\n",
      "[Loss after epoch=560] train loss: 0.558582, test loss: 0.766869\n",
      "[Loss after epoch=570] train loss: 0.556877, test loss: 0.765813\n",
      "[Loss after epoch=580] train loss: 0.555951, test loss: 0.765301\n",
      "[Loss after epoch=590] train loss: 0.555825, test loss: 0.765405\n",
      "[Loss after epoch=600] train loss: 0.556512, test loss: 0.766202\n",
      "[Loss after epoch=610] train loss: 0.554466, test loss: 0.764404\n",
      "[Loss after epoch=620] train loss: 0.552528, test loss: 0.762128\n",
      "[Loss after epoch=630] train loss: 0.551189, test loss: 0.760485\n",
      "[Loss after epoch=640] train loss: 0.550350, test loss: 0.759446\n",
      "[Loss after epoch=650] train loss: 0.549979, test loss: 0.758974\n",
      "[Loss after epoch=660] train loss: 0.550057, test loss: 0.759021\n",
      "[Loss after epoch=670] train loss: 0.547976, test loss: 0.757038\n",
      "[Loss after epoch=680] train loss: 0.546027, test loss: 0.754893\n",
      "[Loss after epoch=690] train loss: 0.544445, test loss: 0.753142\n",
      "[Loss after epoch=700] train loss: 0.543134, test loss: 0.751687\n",
      "[Loss after epoch=710] train loss: 0.542064, test loss: 0.750472\n",
      "[Loss after epoch=720] train loss: 0.541218, test loss: 0.749459\n",
      "[Loss after epoch=730] train loss: 0.540582, test loss: 0.748620\n",
      "[Loss after epoch=740] train loss: 0.540142, test loss: 0.747936\n",
      "[Loss after epoch=750] train loss: 0.539883, test loss: 0.747393\n",
      "[Loss after epoch=760] train loss: 0.539793, test loss: 0.746978\n",
      "[Loss after epoch=770] train loss: 0.539857, test loss: 0.746679\n",
      "[Loss after epoch=780] train loss: 0.538258, test loss: 0.744667\n",
      "[Loss after epoch=790] train loss: 0.536724, test loss: 0.742469\n",
      "[Loss after epoch=800] train loss: 0.535359, test loss: 0.740449\n",
      "[Loss after epoch=810] train loss: 0.534080, test loss: 0.738522\n",
      "[Loss after epoch=820] train loss: 0.532864, test loss: 0.736652\n",
      "[Loss after epoch=830] train loss: 0.531702, test loss: 0.734823\n",
      "[Loss after epoch=840] train loss: 0.530590, test loss: 0.733030\n",
      "[Loss after epoch=850] train loss: 0.529529, test loss: 0.731276\n",
      "[Loss after epoch=860] train loss: 0.528519, test loss: 0.729565\n",
      "[Loss after epoch=870] train loss: 0.527567, test loss: 0.727906\n",
      "[Loss after epoch=880] train loss: 0.526679, test loss: 0.726308\n",
      "[Loss after epoch=890] train loss: 0.525860, test loss: 0.724782\n",
      "[Loss after epoch=900] train loss: 0.525119, test loss: 0.723336\n",
      "[Loss after epoch=910] train loss: 0.524463, test loss: 0.721978\n",
      "[Loss after epoch=920] train loss: 0.523898, test loss: 0.720715\n",
      "[Loss after epoch=930] train loss: 0.523432, test loss: 0.719551\n",
      "[Loss after epoch=940] train loss: 0.523069, test loss: 0.718489\n",
      "[Loss after epoch=950] train loss: 0.522813, test loss: 0.717531\n",
      "[Loss after epoch=960] train loss: 0.522666, test loss: 0.716675\n",
      "[Loss after epoch=970] train loss: 0.522629, test loss: 0.715917\n",
      "[Loss after epoch=980] train loss: 0.522704, test loss: 0.715254\n",
      "[Loss after epoch=990] train loss: 0.521715, test loss: 0.713675\n",
      "[Loss after epoch=1000] train loss: 0.520678, test loss: 0.711872\n",
      "[Loss after epoch=1010] train loss: 0.519706, test loss: 0.710191\n",
      "[Loss after epoch=1020] train loss: 0.518775, test loss: 0.708588\n",
      "[Loss after epoch=1030] train loss: 0.517886, test loss: 0.707046\n",
      "[Loss after epoch=1040] train loss: 0.517045, test loss: 0.705555\n",
      "[Loss after epoch=1050] train loss: 0.516256, test loss: 0.704111\n",
      "[Loss after epoch=1060] train loss: 0.515523, test loss: 0.702711\n",
      "[Loss after epoch=1070] train loss: 0.514850, test loss: 0.701355\n",
      "[Loss after epoch=1080] train loss: 0.514238, test loss: 0.700041\n",
      "[Loss after epoch=1090] train loss: 0.513688, test loss: 0.698769\n",
      "[Loss after epoch=1100] train loss: 0.513203, test loss: 0.697540\n",
      "[Loss after epoch=1110] train loss: 0.512782, test loss: 0.696352\n",
      "[Loss after epoch=1120] train loss: 0.512425, test loss: 0.695207\n",
      "[Loss after epoch=1130] train loss: 0.512132, test loss: 0.694104\n",
      "[Loss after epoch=1140] train loss: 0.511903, test loss: 0.693042\n",
      "[Loss after epoch=1150] train loss: 0.511738, test loss: 0.692022\n",
      "[Loss after epoch=1160] train loss: 0.511635, test loss: 0.691042\n",
      "[Loss after epoch=1170] train loss: 0.511593, test loss: 0.690103\n",
      "[Loss after epoch=1180] train loss: 0.511610, test loss: 0.689204\n",
      "[Loss after epoch=1190] train loss: 0.510993, test loss: 0.687955\n",
      "[Loss after epoch=1200] train loss: 0.510293, test loss: 0.686538\n",
      "[Loss after epoch=1210] train loss: 0.509605, test loss: 0.685173\n",
      "[Loss after epoch=1220] train loss: 0.508924, test loss: 0.683833\n",
      "[Loss after epoch=1230] train loss: 0.508255, test loss: 0.682509\n",
      "[Loss after epoch=1240] train loss: 0.507606, test loss: 0.681200\n",
      "[Loss after epoch=1250] train loss: 0.506979, test loss: 0.679906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loss after epoch=1260] train loss: 0.506378, test loss: 0.678630\n",
      "[Loss after epoch=1270] train loss: 0.505805, test loss: 0.677372\n",
      "[Loss after epoch=1280] train loss: 0.505260, test loss: 0.676134\n",
      "[Loss after epoch=1290] train loss: 0.504745, test loss: 0.674919\n",
      "[Loss after epoch=1300] train loss: 0.504259, test loss: 0.673727\n",
      "[Loss after epoch=1310] train loss: 0.503802, test loss: 0.672559\n",
      "[Loss after epoch=1320] train loss: 0.503375, test loss: 0.671417\n",
      "[Loss after epoch=1330] train loss: 0.502977, test loss: 0.670302\n",
      "[Loss after epoch=1340] train loss: 0.502607, test loss: 0.669213\n",
      "[Loss after epoch=1350] train loss: 0.502266, test loss: 0.668153\n",
      "[Loss after epoch=1360] train loss: 0.501953, test loss: 0.667121\n",
      "[Loss after epoch=1370] train loss: 0.501668, test loss: 0.666117\n",
      "[Loss after epoch=1380] train loss: 0.501409, test loss: 0.665143\n",
      "[Loss after epoch=1390] train loss: 0.501178, test loss: 0.664198\n",
      "[Loss after epoch=1400] train loss: 0.500972, test loss: 0.663282\n",
      "[Loss after epoch=1410] train loss: 0.500792, test loss: 0.662395\n",
      "[Loss after epoch=1420] train loss: 0.500636, test loss: 0.661538\n",
      "[Loss after epoch=1430] train loss: 0.500506, test loss: 0.660711\n",
      "[Loss after epoch=1440] train loss: 0.500399, test loss: 0.659913\n",
      "[Loss after epoch=1450] train loss: 0.500316, test loss: 0.659145\n",
      "[Loss after epoch=1460] train loss: 0.500255, test loss: 0.658406\n",
      "[Loss after epoch=1470] train loss: 0.500217, test loss: 0.657696\n",
      "[Loss after epoch=1480] train loss: 0.500201, test loss: 0.657016\n",
      "[Loss after epoch=1490] train loss: 0.500207, test loss: 0.656365\n",
      "[Loss after epoch=1500] train loss: 0.499823, test loss: 0.655579\n",
      "[Loss after epoch=1510] train loss: 0.499343, test loss: 0.654675\n",
      "[Loss after epoch=1520] train loss: 0.498844, test loss: 0.653791\n",
      "[Loss after epoch=1530] train loss: 0.498334, test loss: 0.652910\n",
      "[Loss after epoch=1540] train loss: 0.497820, test loss: 0.652028\n",
      "[Loss after epoch=1550] train loss: 0.497309, test loss: 0.651151\n",
      "[Loss after epoch=1560] train loss: 0.496804, test loss: 0.650281\n",
      "[Loss after epoch=1570] train loss: 0.496309, test loss: 0.649422\n",
      "[Loss after epoch=1580] train loss: 0.495825, test loss: 0.648577\n",
      "[Loss after epoch=1590] train loss: 0.495354, test loss: 0.647748\n",
      "[Loss after epoch=1600] train loss: 0.494896, test loss: 0.646938\n",
      "[Loss after epoch=1610] train loss: 0.494453, test loss: 0.646147\n",
      "[Loss after epoch=1620] train loss: 0.494024, test loss: 0.645377\n",
      "[Loss after epoch=1630] train loss: 0.493610, test loss: 0.644628\n",
      "[Loss after epoch=1640] train loss: 0.493211, test loss: 0.643900\n",
      "[Loss after epoch=1650] train loss: 0.492827, test loss: 0.643194\n",
      "[Loss after epoch=1660] train loss: 0.492458, test loss: 0.642509\n",
      "[Loss after epoch=1670] train loss: 0.492103, test loss: 0.641846\n",
      "[Loss after epoch=1680] train loss: 0.491764, test loss: 0.641205\n",
      "[Loss after epoch=1690] train loss: 0.491438, test loss: 0.640584\n",
      "[Loss after epoch=1700] train loss: 0.491128, test loss: 0.639985\n",
      "[Loss after epoch=1710] train loss: 0.490831, test loss: 0.639405\n",
      "[Loss after epoch=1720] train loss: 0.490548, test loss: 0.638846\n",
      "[Loss after epoch=1730] train loss: 0.490278, test loss: 0.638306\n",
      "[Loss after epoch=1740] train loss: 0.490022, test loss: 0.637786\n",
      "[Loss after epoch=1750] train loss: 0.489779, test loss: 0.637284\n",
      "[Loss after epoch=1760] train loss: 0.489548, test loss: 0.636800\n",
      "[Loss after epoch=1770] train loss: 0.489329, test loss: 0.636333\n",
      "[Loss after epoch=1780] train loss: 0.489123, test loss: 0.635884\n",
      "[Loss after epoch=1790] train loss: 0.488927, test loss: 0.635451\n",
      "[Loss after epoch=1800] train loss: 0.488743, test loss: 0.635034\n",
      "[Loss after epoch=1810] train loss: 0.488570, test loss: 0.634633\n",
      "[Loss after epoch=1820] train loss: 0.488407, test loss: 0.634247\n",
      "[Loss after epoch=1830] train loss: 0.488253, test loss: 0.633875\n",
      "[Loss after epoch=1840] train loss: 0.488110, test loss: 0.633518\n",
      "[Loss after epoch=1850] train loss: 0.487975, test loss: 0.633174\n",
      "[Loss after epoch=1860] train loss: 0.487850, test loss: 0.632843\n",
      "[Loss after epoch=1870] train loss: 0.487732, test loss: 0.632525\n",
      "[Loss after epoch=1880] train loss: 0.487623, test loss: 0.632220\n",
      "[Loss after epoch=1890] train loss: 0.487522, test loss: 0.631926\n",
      "[Loss after epoch=1900] train loss: 0.487428, test loss: 0.631645\n",
      "[Loss after epoch=1910] train loss: 0.487341, test loss: 0.631375\n",
      "[Loss after epoch=1920] train loss: 0.487261, test loss: 0.631116\n",
      "[Loss after epoch=1930] train loss: 0.487188, test loss: 0.630867\n",
      "[Loss after epoch=1940] train loss: 0.487121, test loss: 0.630630\n",
      "[Loss after epoch=1950] train loss: 0.487059, test loss: 0.630402\n",
      "[Loss after epoch=1960] train loss: 0.487004, test loss: 0.630185\n",
      "[Loss after epoch=1970] train loss: 0.486955, test loss: 0.629977\n",
      "[Loss after epoch=1980] train loss: 0.486911, test loss: 0.629778\n",
      "[Loss after epoch=1990] train loss: 0.486873, test loss: 0.629589\n",
      "[Loss after epoch=2000] train loss: 0.486841, test loss: 0.629409\n",
      "[Loss after epoch=2010] train loss: 0.486814, test loss: 0.629237\n",
      "[Loss after epoch=2020] train loss: 0.486793, test loss: 0.629074\n",
      "[Loss after epoch=2030] train loss: 0.486777, test loss: 0.628918\n",
      "[Loss after epoch=2040] train loss: 0.486768, test loss: 0.628770\n",
      "[Loss after epoch=2050] train loss: 0.486766, test loss: 0.628630\n",
      "[Loss after epoch=2060] train loss: 0.486770, test loss: 0.628497\n",
      "[Loss after epoch=2070] train loss: 0.486554, test loss: 0.628249\n",
      "[Loss after epoch=2080] train loss: 0.486260, test loss: 0.627929\n",
      "[Loss after epoch=2090] train loss: 0.485950, test loss: 0.627617\n",
      "[Loss after epoch=2100] train loss: 0.485632, test loss: 0.627298\n",
      "[Loss after epoch=2110] train loss: 0.485312, test loss: 0.626969\n",
      "[Loss after epoch=2120] train loss: 0.484993, test loss: 0.626631\n",
      "[Loss after epoch=2130] train loss: 0.484678, test loss: 0.626288\n",
      "[Loss after epoch=2140] train loss: 0.484369, test loss: 0.625940\n",
      "[Loss after epoch=2150] train loss: 0.484065, test loss: 0.625590\n",
      "[Loss after epoch=2160] train loss: 0.483769, test loss: 0.625239\n",
      "[Loss after epoch=2170] train loss: 0.483479, test loss: 0.624888\n",
      "[Loss after epoch=2180] train loss: 0.483197, test loss: 0.624539\n",
      "[Loss after epoch=2190] train loss: 0.482922, test loss: 0.624193\n",
      "[Loss after epoch=2200] train loss: 0.482654, test loss: 0.623851\n",
      "[Loss after epoch=2210] train loss: 0.482392, test loss: 0.623514\n",
      "[Loss after epoch=2220] train loss: 0.482137, test loss: 0.623183\n",
      "[Loss after epoch=2230] train loss: 0.481887, test loss: 0.622859\n",
      "[Loss after epoch=2240] train loss: 0.481643, test loss: 0.622543\n",
      "[Loss after epoch=2250] train loss: 0.481404, test loss: 0.622236\n",
      "[Loss after epoch=2260] train loss: 0.481171, test loss: 0.621939\n",
      "[Loss after epoch=2270] train loss: 0.480942, test loss: 0.621651\n",
      "[Loss after epoch=2280] train loss: 0.480717, test loss: 0.621374\n",
      "[Loss after epoch=2290] train loss: 0.480497, test loss: 0.621108\n",
      "[Loss after epoch=2300] train loss: 0.480281, test loss: 0.620852\n",
      "[Loss after epoch=2310] train loss: 0.480069, test loss: 0.620608\n",
      "[Loss after epoch=2320] train loss: 0.479861, test loss: 0.620374\n",
      "[Loss after epoch=2330] train loss: 0.479658, test loss: 0.620150\n",
      "[Loss after epoch=2340] train loss: 0.479458, test loss: 0.619938\n",
      "[Loss after epoch=2350] train loss: 0.479263, test loss: 0.619735\n",
      "[Loss after epoch=2360] train loss: 0.479072, test loss: 0.619542\n",
      "[Loss after epoch=2370] train loss: 0.478885, test loss: 0.619359\n",
      "[Loss after epoch=2380] train loss: 0.478702, test loss: 0.619185\n",
      "[Loss after epoch=2390] train loss: 0.478524, test loss: 0.619020\n",
      "[Loss after epoch=2400] train loss: 0.478350, test loss: 0.618863\n",
      "[Loss after epoch=2410] train loss: 0.478180, test loss: 0.618715\n",
      "[Loss after epoch=2420] train loss: 0.478014, test loss: 0.618575\n",
      "[Loss after epoch=2430] train loss: 0.477853, test loss: 0.618442\n",
      "[Loss after epoch=2440] train loss: 0.477696, test loss: 0.618316\n",
      "[Loss after epoch=2450] train loss: 0.477543, test loss: 0.618197\n",
      "[Loss after epoch=2460] train loss: 0.477395, test loss: 0.618085\n",
      "[Loss after epoch=2470] train loss: 0.477250, test loss: 0.617979\n",
      "[Loss after epoch=2480] train loss: 0.477111, test loss: 0.617879\n",
      "[Loss after epoch=2490] train loss: 0.476975, test loss: 0.617785\n",
      "[Loss after epoch=2500] train loss: 0.476844, test loss: 0.617697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loss after epoch=2510] train loss: 0.476717, test loss: 0.617613\n",
      "[Loss after epoch=2520] train loss: 0.476594, test loss: 0.617535\n",
      "[Loss after epoch=2530] train loss: 0.476475, test loss: 0.617462\n",
      "[Loss after epoch=2540] train loss: 0.476361, test loss: 0.617393\n",
      "[Loss after epoch=2550] train loss: 0.476251, test loss: 0.617329\n",
      "[Loss after epoch=2560] train loss: 0.476145, test loss: 0.617269\n",
      "[Loss after epoch=2570] train loss: 0.476043, test loss: 0.617214\n",
      "[Loss after epoch=2580] train loss: 0.475946, test loss: 0.617162\n",
      "[Loss after epoch=2590] train loss: 0.475852, test loss: 0.617114\n",
      "[Loss after epoch=2600] train loss: 0.475763, test loss: 0.617069\n",
      "[Loss after epoch=2610] train loss: 0.475678, test loss: 0.617029\n",
      "[Loss after epoch=2620] train loss: 0.475598, test loss: 0.616991\n",
      "[Loss after epoch=2630] train loss: 0.475521, test loss: 0.616957\n",
      "[Loss after epoch=2640] train loss: 0.475449, test loss: 0.616927\n",
      "[Loss after epoch=2650] train loss: 0.475380, test loss: 0.616899\n",
      "[Loss after epoch=2660] train loss: 0.475316, test loss: 0.616874\n",
      "[Loss after epoch=2670] train loss: 0.475256, test loss: 0.616853\n",
      "[Loss after epoch=2680] train loss: 0.475201, test loss: 0.616834\n",
      "[Loss after epoch=2690] train loss: 0.475149, test loss: 0.616819\n",
      "[Loss after epoch=2700] train loss: 0.475102, test loss: 0.616806\n",
      "[Loss after epoch=2710] train loss: 0.475059, test loss: 0.616796\n",
      "[Loss after epoch=2720] train loss: 0.475021, test loss: 0.616789\n",
      "[Loss after epoch=2730] train loss: 0.474987, test loss: 0.616785\n",
      "[Loss after epoch=2740] train loss: 0.474957, test loss: 0.616784\n",
      "[Loss after epoch=2750] train loss: 0.474931, test loss: 0.616785\n",
      "[Loss after epoch=2760] train loss: 0.474910, test loss: 0.616790\n",
      "[Loss after epoch=2770] train loss: 0.474893, test loss: 0.616797\n",
      "[Loss after epoch=2780] train loss: 0.474881, test loss: 0.616808\n",
      "[Loss after epoch=2790] train loss: 0.474873, test loss: 0.616821\n",
      "[Loss after epoch=2800] train loss: 0.474870, test loss: 0.616837\n",
      "[Loss after epoch=2810] train loss: 0.474871, test loss: 0.616857\n",
      "[Loss after epoch=2820] train loss: 0.474734, test loss: 0.616784\n",
      "[Loss after epoch=2830] train loss: 0.474547, test loss: 0.616686\n",
      "[Loss after epoch=2840] train loss: 0.474354, test loss: 0.616615\n",
      "[Loss after epoch=2850] train loss: 0.474159, test loss: 0.616548\n",
      "[Loss after epoch=2860] train loss: 0.473964, test loss: 0.616480\n",
      "[Loss after epoch=2870] train loss: 0.473771, test loss: 0.616408\n",
      "[Loss after epoch=2880] train loss: 0.473581, test loss: 0.616333\n",
      "[Loss after epoch=2890] train loss: 0.473394, test loss: 0.616256\n",
      "[Loss after epoch=2900] train loss: 0.473210, test loss: 0.616175\n",
      "[Loss after epoch=2910] train loss: 0.473030, test loss: 0.616092\n",
      "[Loss after epoch=2920] train loss: 0.472854, test loss: 0.616007\n",
      "[Loss after epoch=2930] train loss: 0.472682, test loss: 0.615921\n",
      "[Loss after epoch=2940] train loss: 0.472514, test loss: 0.615834\n",
      "[Loss after epoch=2950] train loss: 0.472350, test loss: 0.615745\n",
      "[Loss after epoch=2960] train loss: 0.472190, test loss: 0.615656\n",
      "[Loss after epoch=2970] train loss: 0.472034, test loss: 0.615567\n",
      "[Loss after epoch=2980] train loss: 0.471882, test loss: 0.615478\n",
      "[Loss after epoch=2990] train loss: 0.471733, test loss: 0.615388\n",
      "[Loss after epoch=3000] train loss: 0.471589, test loss: 0.615298\n",
      "[Loss after epoch=3010] train loss: 0.471449, test loss: 0.615209\n",
      "[Loss after epoch=3020] train loss: 0.471313, test loss: 0.615120\n",
      "[Loss after epoch=3030] train loss: 0.471181, test loss: 0.615032\n",
      "[Loss after epoch=3040] train loss: 0.471052, test loss: 0.614944\n",
      "[Loss after epoch=3050] train loss: 0.470928, test loss: 0.614856\n",
      "[Loss after epoch=3060] train loss: 0.470807, test loss: 0.614769\n",
      "[Loss after epoch=3070] train loss: 0.470690, test loss: 0.614683\n",
      "[Loss after epoch=3080] train loss: 0.470577, test loss: 0.614597\n",
      "[Loss after epoch=3090] train loss: 0.470468, test loss: 0.614512\n",
      "[Loss after epoch=3100] train loss: 0.470363, test loss: 0.614428\n",
      "[Loss after epoch=3110] train loss: 0.470261, test loss: 0.614344\n",
      "[Loss after epoch=3120] train loss: 0.470162, test loss: 0.614261\n",
      "[Loss after epoch=3130] train loss: 0.470068, test loss: 0.614179\n",
      "[Loss after epoch=3140] train loss: 0.469977, test loss: 0.614097\n",
      "[Loss after epoch=3150] train loss: 0.469889, test loss: 0.614016\n",
      "[Loss after epoch=3160] train loss: 0.469805, test loss: 0.613936\n",
      "[Loss after epoch=3170] train loss: 0.469724, test loss: 0.613856\n",
      "[Loss after epoch=3180] train loss: 0.469647, test loss: 0.613777\n",
      "[Loss after epoch=3190] train loss: 0.469573, test loss: 0.613698\n",
      "[Loss after epoch=3200] train loss: 0.469503, test loss: 0.613620\n",
      "[Loss after epoch=3210] train loss: 0.469435, test loss: 0.613543\n",
      "[Loss after epoch=3220] train loss: 0.469371, test loss: 0.613466\n",
      "[Loss after epoch=3230] train loss: 0.469311, test loss: 0.613390\n",
      "[Loss after epoch=3240] train loss: 0.469253, test loss: 0.613315\n",
      "[Loss after epoch=3250] train loss: 0.469199, test loss: 0.613240\n",
      "[Loss after epoch=3260] train loss: 0.469148, test loss: 0.613166\n",
      "[Loss after epoch=3270] train loss: 0.469100, test loss: 0.613092\n",
      "[Loss after epoch=3280] train loss: 0.469055, test loss: 0.613019\n",
      "[Loss after epoch=3290] train loss: 0.469013, test loss: 0.612947\n",
      "[Loss after epoch=3300] train loss: 0.468975, test loss: 0.612875\n",
      "[Loss after epoch=3310] train loss: 0.468939, test loss: 0.612803\n",
      "[Loss after epoch=3320] train loss: 0.468906, test loss: 0.612733\n",
      "[Loss after epoch=3330] train loss: 0.468877, test loss: 0.612663\n",
      "[Loss after epoch=3340] train loss: 0.468850, test loss: 0.612593\n",
      "[Loss after epoch=3350] train loss: 0.468827, test loss: 0.612524\n",
      "[Loss after epoch=3360] train loss: 0.468806, test loss: 0.612456\n",
      "[Loss after epoch=3370] train loss: 0.468788, test loss: 0.612389\n",
      "[Loss after epoch=3380] train loss: 0.468774, test loss: 0.612322\n",
      "[Loss after epoch=3390] train loss: 0.468762, test loss: 0.612255\n",
      "[Loss after epoch=3400] train loss: 0.468753, test loss: 0.612190\n",
      "[Loss after epoch=3410] train loss: 0.468747, test loss: 0.612125\n",
      "[Loss after epoch=3420] train loss: 0.468744, test loss: 0.612060\n",
      "[Loss after epoch=3430] train loss: 0.468744, test loss: 0.611997\n",
      "[Loss after epoch=3440] train loss: 0.468746, test loss: 0.611934\n",
      "[Loss after epoch=3450] train loss: 0.468645, test loss: 0.611785\n",
      "[Loss after epoch=3460] train loss: 0.468505, test loss: 0.611623\n",
      "[Loss after epoch=3470] train loss: 0.468363, test loss: 0.611485\n",
      "[Loss after epoch=3480] train loss: 0.468219, test loss: 0.611354\n",
      "[Loss after epoch=3490] train loss: 0.468076, test loss: 0.611223\n",
      "[Loss after epoch=3500] train loss: 0.467934, test loss: 0.611091\n",
      "[Loss after epoch=3510] train loss: 0.467794, test loss: 0.610957\n",
      "[Loss after epoch=3520] train loss: 0.467656, test loss: 0.610819\n",
      "[Loss after epoch=3530] train loss: 0.467519, test loss: 0.610679\n",
      "[Loss after epoch=3540] train loss: 0.467385, test loss: 0.610536\n",
      "[Loss after epoch=3550] train loss: 0.467253, test loss: 0.610391\n",
      "[Loss after epoch=3560] train loss: 0.467124, test loss: 0.610244\n",
      "[Loss after epoch=3570] train loss: 0.466996, test loss: 0.610094\n",
      "[Loss after epoch=3580] train loss: 0.466871, test loss: 0.609943\n",
      "[Loss after epoch=3590] train loss: 0.466747, test loss: 0.609790\n",
      "[Loss after epoch=3600] train loss: 0.466626, test loss: 0.609636\n",
      "[Loss after epoch=3610] train loss: 0.466507, test loss: 0.609480\n",
      "[Loss after epoch=3620] train loss: 0.466390, test loss: 0.609324\n",
      "[Loss after epoch=3630] train loss: 0.466275, test loss: 0.609167\n",
      "[Loss after epoch=3640] train loss: 0.466163, test loss: 0.609010\n",
      "[Loss after epoch=3650] train loss: 0.466052, test loss: 0.608853\n",
      "[Loss after epoch=3660] train loss: 0.465944, test loss: 0.608695\n",
      "[Loss after epoch=3670] train loss: 0.465838, test loss: 0.608538\n",
      "[Loss after epoch=3680] train loss: 0.465734, test loss: 0.608381\n",
      "[Loss after epoch=3690] train loss: 0.465632, test loss: 0.608224\n",
      "[Loss after epoch=3700] train loss: 0.465533, test loss: 0.608067\n",
      "[Loss after epoch=3710] train loss: 0.465435, test loss: 0.607911\n",
      "[Loss after epoch=3720] train loss: 0.465340, test loss: 0.607756\n",
      "[Loss after epoch=3730] train loss: 0.465246, test loss: 0.607601\n",
      "[Loss after epoch=3740] train loss: 0.465155, test loss: 0.607448\n",
      "[Loss after epoch=3750] train loss: 0.465066, test loss: 0.607295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loss after epoch=3760] train loss: 0.464979, test loss: 0.607143\n",
      "[Loss after epoch=3770] train loss: 0.464895, test loss: 0.606993\n",
      "[Loss after epoch=3780] train loss: 0.464812, test loss: 0.606843\n",
      "[Loss after epoch=3790] train loss: 0.464731, test loss: 0.606695\n",
      "[Loss after epoch=3800] train loss: 0.464653, test loss: 0.606547\n",
      "[Loss after epoch=3810] train loss: 0.464577, test loss: 0.606402\n",
      "[Loss after epoch=3820] train loss: 0.464502, test loss: 0.606257\n",
      "[Loss after epoch=3830] train loss: 0.464430, test loss: 0.606114\n",
      "[Loss after epoch=3840] train loss: 0.464360, test loss: 0.605972\n",
      "[Loss after epoch=3850] train loss: 0.464292, test loss: 0.605831\n",
      "[Loss after epoch=3860] train loss: 0.464226, test loss: 0.605692\n",
      "[Loss after epoch=3870] train loss: 0.464162, test loss: 0.605554\n",
      "[Loss after epoch=3880] train loss: 0.464100, test loss: 0.605418\n",
      "[Loss after epoch=3890] train loss: 0.464040, test loss: 0.605284\n",
      "[Loss after epoch=3900] train loss: 0.463983, test loss: 0.605151\n",
      "[Loss after epoch=3910] train loss: 0.463927, test loss: 0.605019\n",
      "[Loss after epoch=3920] train loss: 0.463873, test loss: 0.604889\n",
      "[Loss after epoch=3930] train loss: 0.463822, test loss: 0.604760\n",
      "[Loss after epoch=3940] train loss: 0.463772, test loss: 0.604634\n",
      "[Loss after epoch=3950] train loss: 0.463724, test loss: 0.604508\n",
      "[Loss after epoch=3960] train loss: 0.463679, test loss: 0.604385\n",
      "[Loss after epoch=3970] train loss: 0.463635, test loss: 0.604263\n",
      "[Loss after epoch=3980] train loss: 0.463594, test loss: 0.604142\n",
      "[Loss after epoch=3990] train loss: 0.463554, test loss: 0.604024\n",
      "[Loss after epoch=4000] train loss: 0.463516, test loss: 0.603906\n",
      "[Loss after epoch=4010] train loss: 0.463481, test loss: 0.603791\n",
      "[Loss after epoch=4020] train loss: 0.463447, test loss: 0.603677\n",
      "[Loss after epoch=4030] train loss: 0.463416, test loss: 0.603565\n",
      "[Loss after epoch=4040] train loss: 0.463386, test loss: 0.603455\n",
      "[Loss after epoch=4050] train loss: 0.463359, test loss: 0.603346\n",
      "[Loss after epoch=4060] train loss: 0.463333, test loss: 0.603239\n",
      "[Loss after epoch=4070] train loss: 0.463309, test loss: 0.603134\n",
      "[Loss after epoch=4080] train loss: 0.463288, test loss: 0.603031\n",
      "[Loss after epoch=4090] train loss: 0.463268, test loss: 0.602929\n",
      "[Loss after epoch=4100] train loss: 0.463251, test loss: 0.602829\n",
      "[Loss after epoch=4110] train loss: 0.463235, test loss: 0.602731\n",
      "[Loss after epoch=4120] train loss: 0.463221, test loss: 0.602635\n",
      "[Loss after epoch=4130] train loss: 0.463210, test loss: 0.602540\n",
      "[Loss after epoch=4140] train loss: 0.463200, test loss: 0.602447\n",
      "[Loss after epoch=4150] train loss: 0.463192, test loss: 0.602356\n",
      "[Loss after epoch=4160] train loss: 0.463186, test loss: 0.602267\n",
      "[Loss after epoch=4170] train loss: 0.463183, test loss: 0.602180\n",
      "[Loss after epoch=4180] train loss: 0.463181, test loss: 0.602094\n",
      "[Loss after epoch=4190] train loss: 0.463181, test loss: 0.602011\n",
      "[Loss after epoch=4200] train loss: 0.463098, test loss: 0.601862\n",
      "[Loss after epoch=4210] train loss: 0.462987, test loss: 0.601708\n",
      "[Loss after epoch=4220] train loss: 0.462873, test loss: 0.601573\n",
      "[Loss after epoch=4230] train loss: 0.462758, test loss: 0.601446\n",
      "[Loss after epoch=4240] train loss: 0.462644, test loss: 0.601322\n",
      "[Loss after epoch=4250] train loss: 0.462531, test loss: 0.601198\n",
      "[Loss after epoch=4260] train loss: 0.462419, test loss: 0.601074\n",
      "[Loss after epoch=4270] train loss: 0.462309, test loss: 0.600950\n",
      "[Loss after epoch=4280] train loss: 0.462200, test loss: 0.600826\n",
      "[Loss after epoch=4290] train loss: 0.462092, test loss: 0.600701\n",
      "[Loss after epoch=4300] train loss: 0.461986, test loss: 0.600577\n",
      "[Loss after epoch=4310] train loss: 0.461882, test loss: 0.600452\n",
      "[Loss after epoch=4320] train loss: 0.461779, test loss: 0.600327\n",
      "[Loss after epoch=4330] train loss: 0.461677, test loss: 0.600202\n",
      "[Loss after epoch=4340] train loss: 0.461577, test loss: 0.600078\n",
      "[Loss after epoch=4350] train loss: 0.461479, test loss: 0.599954\n",
      "[Loss after epoch=4360] train loss: 0.461382, test loss: 0.599832\n",
      "[Loss after epoch=4370] train loss: 0.461286, test loss: 0.599710\n",
      "[Loss after epoch=4380] train loss: 0.461193, test loss: 0.599589\n",
      "[Loss after epoch=4390] train loss: 0.461100, test loss: 0.599469\n",
      "[Loss after epoch=4400] train loss: 0.461009, test loss: 0.599351\n",
      "[Loss after epoch=4410] train loss: 0.460920, test loss: 0.599234\n",
      "[Loss after epoch=4420] train loss: 0.460832, test loss: 0.599118\n",
      "[Loss after epoch=4430] train loss: 0.460746, test loss: 0.599004\n",
      "[Loss after epoch=4440] train loss: 0.460662, test loss: 0.598892\n",
      "[Loss after epoch=4450] train loss: 0.460579, test loss: 0.598782\n",
      "[Loss after epoch=4460] train loss: 0.460497, test loss: 0.598673\n",
      "[Loss after epoch=4470] train loss: 0.460417, test loss: 0.598567\n",
      "[Loss after epoch=4480] train loss: 0.460339, test loss: 0.598462\n",
      "[Loss after epoch=4490] train loss: 0.460262, test loss: 0.598359\n",
      "[Loss after epoch=4500] train loss: 0.460187, test loss: 0.598259\n",
      "[Loss after epoch=4510] train loss: 0.460114, test loss: 0.598160\n",
      "[Loss after epoch=4520] train loss: 0.460042, test loss: 0.598064\n",
      "[Loss after epoch=4530] train loss: 0.459972, test loss: 0.597970\n",
      "[Loss after epoch=4540] train loss: 0.459903, test loss: 0.597878\n",
      "[Loss after epoch=4550] train loss: 0.459836, test loss: 0.597788\n",
      "[Loss after epoch=4560] train loss: 0.459770, test loss: 0.597701\n",
      "[Loss after epoch=4570] train loss: 0.459706, test loss: 0.597616\n",
      "[Loss after epoch=4580] train loss: 0.459644, test loss: 0.597533\n",
      "[Loss after epoch=4590] train loss: 0.459584, test loss: 0.597453\n",
      "[Loss after epoch=4600] train loss: 0.459525, test loss: 0.597375\n",
      "[Loss after epoch=4610] train loss: 0.459467, test loss: 0.597299\n",
      "[Loss after epoch=4620] train loss: 0.459411, test loss: 0.597226\n",
      "[Loss after epoch=4630] train loss: 0.459357, test loss: 0.597155\n",
      "[Loss after epoch=4640] train loss: 0.459305, test loss: 0.597087\n",
      "[Loss after epoch=4650] train loss: 0.459254, test loss: 0.597021\n",
      "[Loss after epoch=4660] train loss: 0.459204, test loss: 0.596958\n",
      "[Loss after epoch=4670] train loss: 0.459157, test loss: 0.596897\n",
      "[Loss after epoch=4680] train loss: 0.459110, test loss: 0.596838\n",
      "[Loss after epoch=4690] train loss: 0.459066, test loss: 0.596782\n",
      "[Loss after epoch=4700] train loss: 0.459023, test loss: 0.596728\n",
      "[Loss after epoch=4710] train loss: 0.458982, test loss: 0.596677\n",
      "[Loss after epoch=4720] train loss: 0.458942, test loss: 0.596628\n",
      "[Loss after epoch=4730] train loss: 0.458904, test loss: 0.596582\n",
      "[Loss after epoch=4740] train loss: 0.458867, test loss: 0.596538\n",
      "[Loss after epoch=4750] train loss: 0.458832, test loss: 0.596497\n",
      "[Loss after epoch=4760] train loss: 0.458799, test loss: 0.596458\n",
      "[Loss after epoch=4770] train loss: 0.458767, test loss: 0.596421\n",
      "[Loss after epoch=4780] train loss: 0.458737, test loss: 0.596387\n",
      "[Loss after epoch=4790] train loss: 0.458709, test loss: 0.596356\n",
      "[Loss after epoch=4800] train loss: 0.458682, test loss: 0.596326\n",
      "[Loss after epoch=4810] train loss: 0.458656, test loss: 0.596299\n",
      "[Loss after epoch=4820] train loss: 0.458632, test loss: 0.596275\n",
      "[Loss after epoch=4830] train loss: 0.458610, test loss: 0.596253\n",
      "[Loss after epoch=4840] train loss: 0.458589, test loss: 0.596234\n",
      "[Loss after epoch=4850] train loss: 0.458570, test loss: 0.596216\n",
      "[Loss after epoch=4860] train loss: 0.458552, test loss: 0.596201\n",
      "[Loss after epoch=4870] train loss: 0.458536, test loss: 0.596189\n",
      "[Loss after epoch=4880] train loss: 0.458522, test loss: 0.596179\n",
      "[Loss after epoch=4890] train loss: 0.458509, test loss: 0.596171\n",
      "[Loss after epoch=4900] train loss: 0.458497, test loss: 0.596166\n",
      "[Loss after epoch=4910] train loss: 0.458487, test loss: 0.596162\n",
      "[Loss after epoch=4920] train loss: 0.458479, test loss: 0.596162\n",
      "[Loss after epoch=4930] train loss: 0.458472, test loss: 0.596163\n",
      "[Loss after epoch=4940] train loss: 0.458467, test loss: 0.596167\n",
      "[Loss after epoch=4950] train loss: 0.458463, test loss: 0.596173\n",
      "[Loss after epoch=4960] train loss: 0.458460, test loss: 0.596181\n",
      "[Loss after epoch=4970] train loss: 0.458459, test loss: 0.596191\n",
      "[Loss after epoch=4980] train loss: 0.458460, test loss: 0.596204\n",
      "[Loss after epoch=4990] train loss: 0.458394, test loss: 0.596164\n",
      "training done\n"
     ]
    }
   ],
   "source": [
    "word_dim = 50\n",
    "hidden_dim = 100\n",
    "\n",
    "np.random.seed(10)\n",
    "rnn = Model(word_dim, hidden_dim)\n",
    "rnn.train(embedded_train_X, train_Y, embedded_test_X, test_Y, learning_rate=0.005, nepoch=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_to_emoji(pred):\n",
    "    for i in pred:\n",
    "        print(utils.label_to_emoji(i), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN model accuracy for test set is 70.28%\n"
     ]
    }
   ],
   "source": [
    "pred, acc = rnn.predict(embedded_test_X, test_Y)\n",
    "print(\"RNN model accuracy for test set is \"+ str(acc)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🍴 😞 😄 😄 😞 😄 😞 😄 🍴 😄 ⚾ 😞 😞 😞 ⚾ 😞 😞 ❤️ 😞 🍴 ❤️ 😄 🍴 😞 😞 😞 ⚾ ❤️ ⚾ 😄 ❤️ ⚾ 😞 😄 😄 ⚾ 😞 🍴 🍴 😄 ⚾ ❤️ ❤️ ⚾ 😞 😞 😄 😄 😄 😞 😞 ❤️ 😞 😞 😄 🍴 "
     ]
    }
   ],
   "source": [
    "pred_to_emoji(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rnn.visualize_loss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
