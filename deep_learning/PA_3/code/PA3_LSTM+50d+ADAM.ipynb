{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import emo_utils as utils\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import deque\n",
    "from scipy.special import logsumexp\n",
    "import math\n",
    "\n",
    "def word_embedding(train_X):\n",
    "    new_train_X = []\n",
    "    for sentence in train_X:\n",
    "        temp = sentence.split()\n",
    "        blank = [x*0 for x in range(50)]\n",
    "        result = []\n",
    "        for word in temp:\n",
    "            word = word.lower()\n",
    "            result.append(word_to_vec_map[word])\n",
    "        while len(result) < 10:\n",
    "            result.append(blank)\n",
    "        new_train_X.append(result)\n",
    "    return new_train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = utils.read_csv(\"train_emoji.csv\")\n",
    "test_X, test_Y = utils.read_csv(\"test_emoji.csv\")\n",
    "words_to_index, index_to_words, word_to_vec_map = utils.read_glove_vecs(\"glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_train_X = word_embedding(train_X)\n",
    "embedded_test_X = word_embedding(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50)\n",
      "(132, 10, 50)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(embedded_train_X[0]).shape)\n",
    "print(np.array(embedded_train_X).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find Linear\n",
    "class Linear:\n",
    "    def __init__(self, input_size, output_size, learning_rate = 0.01):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.W = np.random.randn(input_size, output_size)*0.01\n",
    "        self.b = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, S):\n",
    "        self.input = S\n",
    "        self.output = np.dot(S, self.W) + self.b\n",
    "        return  self.output\n",
    "    \n",
    "    def backward(self, Z):\n",
    "        S = self.input\n",
    "        gradient = np.dot(Z, self.W.T)\n",
    "        delta_W = np.dot(S.T, Z)\n",
    "        delta_b  = Z.mean(axis=0)*S.shape[0]\n",
    "        self.W = self.W - self.learning_rate * delta_W  \n",
    "        self.b = self.b - self.learning_rate * delta_b\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(x):\n",
    "    return 1 - np.tanh(x) * np.tanh(x)\n",
    "\n",
    "def softmax(x, axis=None):\n",
    "    return np.exp(x - logsumexp(x, axis=axis, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNOptimizer(object):\n",
    "    def __init__(self,lr = 0.01,gradient_clipping = True):\n",
    "        self.lr = lr\n",
    "        self.gradient_clipping = gradient_clipping\n",
    "        self.first = True\n",
    "\n",
    "    def step(self):\n",
    "        for layer in self.model.layers:\n",
    "            for key in layer.params.keys():\n",
    "                if self.gradient_clipping:\n",
    "                    np.clip(layer.params[key]['deriv'], -2, 2, layer.params[key]['deriv'])\n",
    "\n",
    "                self._update_rule(param=layer.params[key]['value'], grad=layer.params[key]['deriv'])\n",
    "\n",
    "    def _update_rule(self, **kwargs):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(RNNOptimizer):\n",
    "    def __init__(self,\n",
    "                 lr= 0.01,\n",
    "                 gradient_clipping = True):\n",
    "        super().__init__(lr, gradient_clipping)\n",
    "\n",
    "    def _update_rule(self, **kwargs):\n",
    "        update = self.lr*kwargs['grad']\n",
    "        kwargs['param'] -= update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADAM(RNNOptimizer):\n",
    "    def __init__(self, lr=0.01, betas=(0.9, 0.99), eps=1e-8, weight_decay=0):\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.state = []\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        \n",
    "    def step(self):\n",
    "        loss = None\n",
    "        for i, layer in enumerate(self.model.layers):\n",
    "            for key in layer.params.keys():\n",
    "                self._update_rule(param=layer.params[key]['value'], grad=layer.params[key]['deriv'])\n",
    "                \n",
    "    def _update_rule(self, **kwargs):\n",
    "        grad = kwargs['grad']\n",
    "        state = {}\n",
    "        if len(state) == 0:\n",
    "            state['step'] = 0\n",
    "            state['exp_avg'] = np.zeros_like(kwargs['param'])\n",
    "            state['exp_avg_sq'] = np.zeros_like(kwargs['param'])\n",
    "\n",
    "        exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "\n",
    "        b1, b2 = self.betas\n",
    "        state['step'] += 1\n",
    "\n",
    "        exp_avg = np.dot(exp_avg, b1) + (1 - b1)*grad\n",
    "        exp_avg_sq = np.dot(exp_avg_sq, b2) + (1-b2)*(grad*grad)\n",
    "\n",
    "        denom = np.sqrt(exp_avg_sq) + self.eps\n",
    "\n",
    "        bias_correction1 = 1 / (1 - b1 ** state['step'])\n",
    "        bias_correction2 = 1 / (1 - b2 ** state['step'])\n",
    "\n",
    "        adapted_learning_rate = self.lr * bias_correction1 / math.sqrt(bias_correction2)\n",
    "\n",
    "        kwargs['param'] = kwargs['param'] - adapted_learning_rate * exp_avg / denom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropy:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, result, Y):\n",
    "        answers = result[np.arange(len(result)),Y]\n",
    "        cross_entropy = - answers + np.log(np.sum(np.exp(result),axis=-1))\n",
    "        return cross_entropy\n",
    "\n",
    "    def backward(self, result, Y):\n",
    "        ones_for_answers = np.zeros_like(result)\n",
    "        ones_for_answers[np.arange(len(result)),Y] = 1    \n",
    "        softmax = np.exp(result) / np.exp(result).sum(axis=-1,keepdims=True)\n",
    "        return (- ones_for_answers + softmax) / result.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNode:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, X_in, H_in, C_in, params_dict):\n",
    "        self.X_in = X_in\n",
    "        self.C_in = C_in\n",
    "\n",
    "        self.Z = np.column_stack((X_in, H_in))\n",
    "        \n",
    "        self.f_int = np.dot(self.Z, params_dict['W_f']['value']) + params_dict['B_f']['value']\n",
    "        self.f = sigmoid(self.f_int)\n",
    "        \n",
    "        self.i_int = np.dot(self.Z, params_dict['W_i']['value']) + params_dict['B_i']['value']\n",
    "        self.i = sigmoid(self.i_int)\n",
    "        self.C_bar_int = np.dot(self.Z, params_dict['W_c']['value']) + params_dict['B_c']['value']\n",
    "        self.C_bar = tanh(self.C_bar_int)\n",
    "\n",
    "        self.C_out = self.f * C_in + self.i * self.C_bar\n",
    "        self.o_int = np.dot(self.Z, params_dict['W_o']['value']) + params_dict['B_o']['value']\n",
    "        self.o = sigmoid(self.o_int)\n",
    "        self.H_out = self.o * tanh(self.C_out)\n",
    "\n",
    "        self.X_out = np.dot(self.H_out, params_dict['W_v']['value']) + params_dict['B_v']['value']\n",
    "        \n",
    "        return self.X_out, self.H_out, self.C_out \n",
    "\n",
    "\n",
    "    def backward(self, X_out_grad, H_out_grad, C_out_grad, params_dict):\n",
    "        params_dict['W_v']['deriv'] += np.dot(self.H_out.T, X_out_grad)\n",
    "        params_dict['B_v']['deriv'] += X_out_grad.sum(axis=0)\n",
    "\n",
    "        dh_out = np.dot(X_out_grad, params_dict['W_v']['value'].T)        \n",
    "        dh_out += H_out_grad\n",
    "                         \n",
    "        do = dh_out * tanh(self.C_out)\n",
    "        do_int = dsigmoid(self.o_int) * do\n",
    "        params_dict['W_o']['deriv'] += np.dot(self.Z.T, do_int)\n",
    "        params_dict['B_o']['deriv'] += do_int.sum(axis=0)\n",
    "\n",
    "        dC_out = dh_out * self.o * dtanh(self.C_out)\n",
    "        dC_out += C_out_grad\n",
    "        dC_bar = dC_out * self.i\n",
    "        dC_bar_int = dtanh(self.C_bar_int) * dC_bar\n",
    "        params_dict['W_c']['deriv'] += np.dot(self.Z.T, dC_bar_int)\n",
    "        params_dict['B_c']['deriv'] += dC_bar_int.sum(axis=0)\n",
    "\n",
    "        di = dC_out * self.C_bar\n",
    "        di_int = dsigmoid(self.i_int) * di\n",
    "        params_dict['W_i']['deriv'] += np.dot(self.Z.T, di_int)\n",
    "        params_dict['B_i']['deriv'] += di_int.sum(axis=0)\n",
    "\n",
    "        df = dC_out * self.C_in\n",
    "        df_int = dsigmoid(self.f_int) * df\n",
    "        params_dict['W_f']['deriv'] += np.dot(self.Z.T, df_int)\n",
    "        params_dict['B_f']['deriv'] += df_int.sum(axis=0)\n",
    "\n",
    "        dz = (np.dot(df_int, params_dict['W_f']['value'].T)\n",
    "             + np.dot(di_int, params_dict['W_i']['value'].T)\n",
    "             + np.dot(dC_bar_int, params_dict['W_c']['value'].T)\n",
    "             + np.dot(do_int, params_dict['W_o']['value'].T))\n",
    "    \n",
    "        dx_prev = dz[:, :self.X_in.shape[1]]\n",
    "        dH_prev = dz[:, self.X_in.shape[1]:]\n",
    "        dC_prev = self.f * dC_out\n",
    "\n",
    "        return dx_prev, dH_prev, dC_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer:\n",
    "    def __init__(self,hidden_size,output_size,weight_scale = 0.01):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weight_scale = weight_scale\n",
    "        self.start_H = np.zeros((1, hidden_size))\n",
    "        self.start_C = np.zeros((1, hidden_size))        \n",
    "        self.first = True\n",
    "        self.first_backward= True\n",
    "        \n",
    "    def _init_params(self, input_):\n",
    "        self.vocab_size = input_.shape[2]\n",
    "        self.params = {}\n",
    "        self.params['W_f'] = {}\n",
    "        self.params['B_f'] = {}\n",
    "        self.params['W_i'] = {}\n",
    "        self.params['B_i'] = {}\n",
    "        self.params['W_c'] = {}\n",
    "        self.params['B_c'] = {}\n",
    "        self.params['W_o'] = {}\n",
    "        self.params['B_o'] = {}        \n",
    "        self.params['W_v'] = {}\n",
    "        self.params['B_v'] = {}\n",
    "        \n",
    "        self.params['W_f']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size =(self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_f']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size=(1, self.hidden_size))\n",
    "        self.params['W_i']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size=(self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_i']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size=(1, self.hidden_size))\n",
    "        self.params['W_c']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size=(self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_c']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size=(1, self.hidden_size))\n",
    "        self.params['W_o']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size=(self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_o']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size=(1, self.hidden_size))       \n",
    "        self.params['W_v']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size=(self.hidden_size, self.output_size))\n",
    "        self.params['B_v']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size=(1, self.output_size))\n",
    "        \n",
    "        for key in self.params.keys():\n",
    "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['value'])\n",
    "        self.cells = [LSTMNode() for x in range(input_.shape[1])]\n",
    "\n",
    "    def _clear_gradients(self):\n",
    "        for key in self.params.keys():\n",
    "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['deriv'])                    \n",
    "        \n",
    "    def forward(self, x_seq_in):\n",
    "        self.first_backward= True\n",
    "        if self.first:\n",
    "            self._init_params(x_seq_in)\n",
    "            self.first=False\n",
    "        \n",
    "        batch_size = x_seq_in.shape[0]\n",
    "        \n",
    "        H_in = np.copy(self.start_H)\n",
    "        C_in = np.copy(self.start_C)\n",
    "        \n",
    "        H_in = np.repeat(H_in, batch_size, axis=0)\n",
    "        C_in = np.repeat(C_in, batch_size, axis=0)        \n",
    "\n",
    "        length = x_seq_in.shape[1]\n",
    "        \n",
    "        x_seq_out = np.zeros((batch_size, length, self.output_size))\n",
    "        \n",
    "        for t in range(length):\n",
    "            x_in = x_seq_in[:, t, :]\n",
    "            y_out, H_in, C_in = self.cells[t].forward(x_in, H_in, C_in, self.params)\n",
    "            x_seq_out[:, t, :] = y_out\n",
    "    \n",
    "        self.start_H = H_in.mean(axis=0, keepdims=True)\n",
    "        self.start_C = C_in.mean(axis=0, keepdims=True)        \n",
    "        \n",
    "        return x_seq_out, y_out\n",
    "\n",
    "\n",
    "    def backward(self, x_seq_out_grad):\n",
    "        batch_size = x_seq_out_grad.shape[0]\n",
    "        \n",
    "        h_in_grad = np.zeros((batch_size, self.hidden_size))\n",
    "        c_in_grad = np.zeros((batch_size, self.hidden_size))        \n",
    "        \n",
    "        num_chars = x_seq_out_grad.shape[1]\n",
    "        \n",
    "        x_seq_in_grad = np.zeros((batch_size, num_chars, self.vocab_size))\n",
    "        \n",
    "        for t in reversed(range(10)):\n",
    "            x_out_grad = x_seq_out_grad\n",
    "            grad_out, h_in_grad, c_in_grad = self.cells[t].backward(x_out_grad, h_in_grad, c_in_grad, self.params)\n",
    "            x_seq_in_grad[:, t, :] = grad_out\n",
    "        return x_seq_in_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, layers, sequence_length, vocab_size, hidden_size,loss):\n",
    "        self.layers = layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.loss = loss\n",
    "        self.Linear = Linear(vocab_size, 5)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            setattr(layer, 'sequence_length', sequence_length)\n",
    "\n",
    "    def forward(self, x_batch):  \n",
    "        for layer in self.layers:\n",
    "            total, x_batch = layer.forward(x_batch)\n",
    "        #print(\"total\", total.shape)\n",
    "        #print(\"x_batch after LSTM\", x_batch.shape)\n",
    "        \n",
    "        x_batch = self.Linear.forward(x_batch)\n",
    "        #print(\"x_batch atfer Linear\", x_batch.shape)\n",
    "        return x_batch\n",
    "        \n",
    "    def backward(self, loss_grad):\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_grad = layer.backward(loss_grad)\n",
    "            \n",
    "        return loss_grad\n",
    "                \n",
    "    def single_step(self, x_batch, y_batch):\n",
    "\n",
    "        x_batch_out = self.forward(x_batch)\n",
    "        #print(\"x_batch_out\", x_batch_out.shape)\n",
    "        predict, _ = self.predict(x_batch_out, y_batch)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            layer._clear_gradients()\n",
    "            \n",
    "        loss = self.loss.forward(x_batch_out, y_batch)\n",
    "        loss_grad = self.loss.backward(x_batch_out, y_batch)\n",
    "        loss_grad = self.Linear.backward(loss_grad)\n",
    "        self.backward(loss_grad)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def predict(self, X, Y):\n",
    "        pred = []\n",
    "        for probs in X:\n",
    "            pred.append(np.argmax(probs))\n",
    "        acc = 0\n",
    "        for i in range(len(Y)):\n",
    "            if pred[i] == Y[i]:\n",
    "                acc += 1\n",
    "        acc = acc/len(Y) * 100\n",
    "        return pred, acc\n",
    "    \n",
    "    def evaluate(self, x_batch, y_batch):\n",
    "        x_batch_out = self.forward(x_batch)\n",
    "        loss = self.loss.forward(x_batch_out, y_batch)\n",
    "        return loss\n",
    "    \n",
    "    def make_result(self, X, Y):\n",
    "        X_out = self.forward(X)\n",
    "        pred = []\n",
    "        for x in X_out:\n",
    "            pred.append(np.argmax(x))\n",
    "        acc = 0\n",
    "        for i in range(len(Y)):\n",
    "            if pred[i] == Y[i]:\n",
    "                acc += 1\n",
    "        acc = acc/len(Y) * 100\n",
    "        return pred, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTrainer:\n",
    "    def __init__(self, train_X, train_Y, test_X, test_Y, model,optim):\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y\n",
    "        self.model = model\n",
    "        self.sequence_length = self.model.sequence_length\n",
    "        self.optim = optim\n",
    "        self.train_loss_log = []\n",
    "        self.test_loss_log = []\n",
    "        self.log_step = []\n",
    "        setattr(self.optim, 'model', self.model)\n",
    "\n",
    "    def visualize_loss(self):\n",
    "        plt.plot(self.log_step, self.train_loss_log, label='train loss')\n",
    "        plt.plot(self.log_step, self.test_loss_log, label='validation loss')\n",
    "        plt.xlabel(\"num iteration\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.legend(fontsize='x-large')\n",
    "        \n",
    "    def train(self, num_iterations):\n",
    "        train_X = self.train_X\n",
    "        train_Y = self.train_Y\n",
    "        test_X = self.test_X\n",
    "        test_Y = self.test_Y \n",
    "        \n",
    "        num_iter = 0\n",
    "        \n",
    "        train_moving_average = deque(maxlen=100)\n",
    "        test_moving_average = deque(maxlen=100)\n",
    "        \n",
    "        while num_iter < num_iterations:\n",
    "            inputs_batch = np.array(train_X)\n",
    "            targets_batch = np.array(train_Y)\n",
    "            train_loss = self.model.single_step(inputs_batch, targets_batch)\n",
    "            test_loss = self.model.evaluate(np.array(test_X), np.array(test_Y))\n",
    "            train_moving_average.append(train_loss)\n",
    "            train_ma_loss = np.mean(train_moving_average)            \n",
    "            test_moving_average.append(test_loss)\n",
    "            test_ma_loss = np.mean(test_moving_average)\n",
    "            self.optim.step()\n",
    "            self.train_loss_log.append(train_ma_loss)\n",
    "            self.test_loss_log.append(test_ma_loss)\n",
    "            self.log_step.append(num_iter)\n",
    "            if num_iter % 100 == 0:\n",
    "                print(\"[Loss after %d iter] train loss: %f, test loss: %f\" % (num_iter, train_ma_loss, test_ma_loss))\n",
    "            num_iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loss after 0 iter] train loss: 1.609579, test loss: 1.609271\n",
      "[Loss after 100 iter] train loss: 1.595959, test loss: 1.593721\n",
      "[Loss after 200 iter] train loss: 1.574496, test loss: 1.568690\n",
      "[Loss after 300 iter] train loss: 1.558972, test loss: 1.549760\n",
      "[Loss after 400 iter] train loss: 1.549047, test loss: 1.536735\n",
      "[Loss after 500 iter] train loss: 1.544327, test loss: 1.529709\n",
      "[Loss after 600 iter] train loss: 1.542651, test loss: 1.526713\n",
      "[Loss after 700 iter] train loss: 1.542069, test loss: 1.525554\n",
      "[Loss after 800 iter] train loss: 1.541737, test loss: 1.525073\n",
      "[Loss after 900 iter] train loss: 1.541390, test loss: 1.524800\n",
      "[Loss after 1000 iter] train loss: 1.540907, test loss: 1.524521\n",
      "[Loss after 1100 iter] train loss: 1.540138, test loss: 1.524055\n",
      "[Loss after 1200 iter] train loss: 1.538679, test loss: 1.523016\n",
      "[Loss after 1300 iter] train loss: 1.534669, test loss: 1.519724\n",
      "[Loss after 1400 iter] train loss: 1.499707, test loss: 1.491308\n",
      "[Loss after 1500 iter] train loss: 1.310441, test loss: 1.373118\n",
      "[Loss after 1600 iter] train loss: 1.193549, test loss: 1.299340\n",
      "[Loss after 1700 iter] train loss: 1.101079, test loss: 1.234542\n",
      "[Loss after 1800 iter] train loss: 1.039647, test loss: 1.229062\n",
      "[Loss after 1900 iter] train loss: 0.975942, test loss: 1.252624\n",
      "[Loss after 2000 iter] train loss: 0.947571, test loss: 1.314918\n",
      "[Loss after 2100 iter] train loss: 0.935753, test loss: 1.352928\n",
      "[Loss after 2200 iter] train loss: 0.928567, test loss: 1.394056\n",
      "[Loss after 2300 iter] train loss: 0.829935, test loss: 1.362432\n",
      "[Loss after 2400 iter] train loss: 0.696639, test loss: 1.293300\n",
      "[Loss after 2500 iter] train loss: 0.567763, test loss: 1.284779\n",
      "[Loss after 2600 iter] train loss: 0.482278, test loss: 1.361695\n",
      "[Loss after 2700 iter] train loss: 0.427038, test loss: 1.422141\n",
      "[Loss after 2800 iter] train loss: 0.369574, test loss: 1.374887\n",
      "[Loss after 2900 iter] train loss: 0.431693, test loss: 1.349961\n",
      "[Loss after 3000 iter] train loss: 0.276947, test loss: 1.188140\n"
     ]
    }
   ],
   "source": [
    "layers = [LSTMLayer(hidden_size=256, output_size=50, weight_scale=0.01)]\n",
    "model = Model(layers=layers, vocab_size=50, hidden_size=256, sequence_length=10,loss=SoftmaxCrossEntropy())\n",
    "optimizer = ADAM()\n",
    "\n",
    "RNN = RNNTrainer(embedded_train_X, train_Y, embedded_test_X, test_Y, model, optimizer)\n",
    "RNN.train(3001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_to_emoji(pred):\n",
    "    for i in pred:\n",
    "        print(utils.label_to_emoji(i), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN model accuracy for test set is 78.57%\n"
     ]
    }
   ],
   "source": [
    "pred, acc = RNN.model.make_result(np.array(embedded_test_X), test_Y)\n",
    "print(\"RNN model accuracy for test set is \"+ str(round(acc,2))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ´ ðŸ˜ž ðŸ˜„ ðŸ˜„ ðŸ˜„ ðŸ˜„ ðŸ˜ž ðŸ˜„ ðŸ´ ðŸ˜„ âš¾ ðŸ˜ž â¤ï¸ ðŸ˜ž âš¾ ðŸ˜ž ðŸ˜„ ðŸ˜„ âš¾ ðŸ˜„ ðŸ˜ž ðŸ˜„ ðŸ´ ðŸ˜ž ðŸ˜ž ðŸ˜ž âš¾ â¤ï¸ âš¾ ðŸ˜„ â¤ï¸ âš¾ â¤ï¸ ðŸ˜„ ðŸ˜„ ðŸ˜ž ðŸ˜„ ðŸ´ ðŸ´ ðŸ˜„ âš¾ â¤ï¸ â¤ï¸ âš¾ ðŸ˜„ ðŸ˜ž ðŸ˜„ ðŸ˜„ ðŸ˜ž âš¾ âš¾ ðŸ˜ž ðŸ˜ž ðŸ˜„ ðŸ˜„ ðŸ˜„ "
     ]
    }
   ],
   "source": [
    "pred_to_emoji(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "RNN.visualize_loss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
