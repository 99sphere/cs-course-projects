{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import emo_utils as utils\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import deque\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "def word_embedding(train_X):\n",
    "    new_train_X = []\n",
    "    for sentence in train_X:\n",
    "        temp = sentence.split()\n",
    "        blank = [x*0 for x in range(50)]\n",
    "        result = []\n",
    "        for word in temp:\n",
    "            word = word.lower()\n",
    "            result.append(word_to_vec_map[word])\n",
    "        while len(result) < 10:\n",
    "            result.append(blank)\n",
    "        new_train_X.append(result)\n",
    "    return new_train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = utils.read_csv(\"train_emoji.csv\")\n",
    "test_X, test_Y = utils.read_csv(\"test_emoji.csv\")\n",
    "words_to_index, index_to_words, word_to_vec_map = utils.read_glove_vecs(\"glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_train_X = word_embedding(train_X)\n",
    "embedded_test_X = word_embedding(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50)\n",
      "(132, 10, 50)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(embedded_train_X[0]).shape)\n",
    "print(np.array(embedded_train_X).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find Linear\n",
    "class Linear:\n",
    "    def __init__(self, input_size, output_size, learning_rate = 0.01):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.W = np.random.randn(input_size, output_size)*0.01\n",
    "        self.b = np.zeros((1, output_size))\n",
    "\n",
    "    def dropout_forward(self, X, dropout=0.1):\n",
    "        assert 0 <= dropout <= 1\n",
    "        # In this case, all elements are dropped out\n",
    "        if dropout == 1:\n",
    "            return np.zeros_like(X)\n",
    "        # In this case, all elements are kept\n",
    "        if dropout == 0:\n",
    "            return X\n",
    "        \n",
    "        self.mask = np.random.uniform(0, 1, X.shape) > dropout\n",
    "        return self.mask.astype(np.float32) * X / (1.0 - dropout)\n",
    "\n",
    "    def dropout_backward(self, Z):\n",
    "        gradient = Z * self.mask\n",
    "        return gradient\n",
    "    \n",
    "    def forward(self, S):\n",
    "        self.input = S\n",
    "        self.output = np.dot(S, self.W) + self.b\n",
    "        self.output = self.dropout_forward(self.output)\n",
    "        return  self.output\n",
    "\n",
    "    \n",
    "    def backward(self, Z):\n",
    "        S = self.input\n",
    "        Z = self.dropout_backward(Z)\n",
    "        gradient = np.dot(Z, self.W.T)\n",
    "        delta_W = np.dot(S.T, Z)\n",
    "        delta_b  = Z.mean(axis=0)*S.shape[0]\n",
    "        self.W = self.W - self.learning_rate * delta_W  \n",
    "        self.b = self.b - self.learning_rate * delta_b\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def dsigmoid(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def dtanh(x):\n",
    "    return 1 - np.tanh(x) * np.tanh(x)\n",
    "\n",
    "def softmax(x, axis=None):\n",
    "    return np.exp(x - logsumexp(x, axis=axis, keepdims=True))\n",
    "\n",
    "def batch_softmax(input_array):\n",
    "    out = []\n",
    "    for row in input_array:\n",
    "        out.append(softmax(row, axis=1))\n",
    "    return np.stack(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNOptimizer(object):\n",
    "    def __init__(self,lr = 0.01,gradient_clipping = True):\n",
    "        self.lr = lr\n",
    "        self.gradient_clipping = gradient_clipping\n",
    "        self.first = True\n",
    "\n",
    "    def step(self):\n",
    "        for layer in self.model.layers:\n",
    "            for key in layer.params.keys():\n",
    "                if self.gradient_clipping:\n",
    "                    np.clip(layer.params[key]['deriv'], -2, 2, layer.params[key]['deriv'])\n",
    "\n",
    "                self._update_rule(param=layer.params[key]['value'], grad=layer.params[key]['deriv'])\n",
    "\n",
    "    def _update_rule(self, **kwargs):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(RNNOptimizer):\n",
    "    def __init__(self,\n",
    "                 lr= 0.01,\n",
    "                 gradient_clipping = True):\n",
    "        super().__init__(lr, gradient_clipping)\n",
    "\n",
    "    def _update_rule(self, **kwargs):\n",
    "        update = self.lr*kwargs['grad']\n",
    "        kwargs['param'] -= update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropy:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, result, Y):\n",
    "        answers = result[np.arange(len(result)),Y]\n",
    "        cross_entropy = - answers + np.log(np.sum(np.exp(result),axis=-1))\n",
    "        return cross_entropy\n",
    "\n",
    "    def backward(self, result, Y):\n",
    "        ones_for_answers = np.zeros_like(result)\n",
    "        ones_for_answers[np.arange(len(result)),Y] = 1    \n",
    "        softmax = np.exp(result) / np.exp(result).sum(axis=-1,keepdims=True)\n",
    "        return (- ones_for_answers + softmax) / result.shape[0]\n",
    "\n",
    "    def _output(self):\n",
    "        self.softmax_preds = np.clip(self.prediction, self.eps, 1 - self.eps)\n",
    "        softmax_cross_entropy_loss = -1.0 * self.target * np.log(self.softmax_preds) - (1.0 - self.target) * np.log(1 - self.softmax_preds)\n",
    "\n",
    "        return np.sum(softmax_cross_entropy_loss)\n",
    "\n",
    "    def _input_grad(self):\n",
    "        return self.softmax_preds - self.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNode:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, X_in, H_in, C_in, params_dict):\n",
    "        self.X_in = X_in\n",
    "        self.C_in = C_in\n",
    "\n",
    "        self.Z = np.column_stack((X_in, H_in))\n",
    "        \n",
    "        self.f_int = np.dot(self.Z, params_dict['W_f']['value']) + params_dict['B_f']['value']\n",
    "        self.f = sigmoid(self.f_int)\n",
    "        \n",
    "        self.i_int = np.dot(self.Z, params_dict['W_i']['value']) + params_dict['B_i']['value']\n",
    "        self.i = sigmoid(self.i_int)\n",
    "        self.C_bar_int = np.dot(self.Z, params_dict['W_c']['value']) + params_dict['B_c']['value']\n",
    "        self.C_bar = tanh(self.C_bar_int)\n",
    "\n",
    "        self.C_out = self.f * C_in + self.i * self.C_bar\n",
    "        self.o_int = np.dot(self.Z, params_dict['W_o']['value']) + params_dict['B_o']['value']\n",
    "        self.o = sigmoid(self.o_int)\n",
    "        self.H_out = self.o * tanh(self.C_out)\n",
    "\n",
    "        self.X_out = np.dot(self.H_out, params_dict['W_v']['value']) + params_dict['B_v']['value']\n",
    "        \n",
    "        return self.X_out, self.H_out, self.C_out \n",
    "\n",
    "\n",
    "    def backward(self, X_out_grad, H_out_grad, C_out_grad, params_dict):\n",
    "        params_dict['W_v']['deriv'] += np.dot(self.H_out.T, X_out_grad)\n",
    "        params_dict['B_v']['deriv'] += X_out_grad.sum(axis=0)\n",
    "\n",
    "        dh_out = np.dot(X_out_grad, params_dict['W_v']['value'].T)        \n",
    "        dh_out += H_out_grad\n",
    "                         \n",
    "        do = dh_out * tanh(self.C_out)\n",
    "        do_int = dsigmoid(self.o_int) * do\n",
    "        params_dict['W_o']['deriv'] += np.dot(self.Z.T, do_int)\n",
    "        params_dict['B_o']['deriv'] += do_int.sum(axis=0)\n",
    "\n",
    "        dC_out = dh_out * self.o * dtanh(self.C_out)\n",
    "        dC_out += C_out_grad\n",
    "        dC_bar = dC_out * self.i\n",
    "        dC_bar_int = dtanh(self.C_bar_int) * dC_bar\n",
    "        params_dict['W_c']['deriv'] += np.dot(self.Z.T, dC_bar_int)\n",
    "        params_dict['B_c']['deriv'] += dC_bar_int.sum(axis=0)\n",
    "\n",
    "        di = dC_out * self.C_bar\n",
    "        di_int = dsigmoid(self.i_int) * di\n",
    "        params_dict['W_i']['deriv'] += np.dot(self.Z.T, di_int)\n",
    "        params_dict['B_i']['deriv'] += di_int.sum(axis=0)\n",
    "\n",
    "        df = dC_out * self.C_in\n",
    "        df_int = dsigmoid(self.f_int) * df\n",
    "        params_dict['W_f']['deriv'] += np.dot(self.Z.T, df_int)\n",
    "        params_dict['B_f']['deriv'] += df_int.sum(axis=0)\n",
    "\n",
    "        dz = (np.dot(df_int, params_dict['W_f']['value'].T)+ np.dot(di_int, params_dict['W_i']['value'].T)+ np.dot(dC_bar_int, params_dict['W_c']['value'].T) + np.dot(do_int, params_dict['W_o']['value'].T))\n",
    "    \n",
    "        dx_prev = dz[:, :self.X_in.shape[1]]\n",
    "        dH_prev = dz[:, self.X_in.shape[1]:]\n",
    "        dC_prev = self.f * dC_out\n",
    "\n",
    "        return dx_prev, dH_prev, dC_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer:\n",
    "    def __init__(self,hidden_size,output_size,weight_scale = 0.01):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weight_scale = weight_scale\n",
    "        self.start_H = np.zeros((1, hidden_size))\n",
    "        self.start_C = np.zeros((1, hidden_size))        \n",
    "        self.first = True\n",
    "        self.first_backward= True\n",
    "\n",
    "    def _init_params(self,input_):\n",
    "        self.vocab_size = input_.shape[2]\n",
    "        self.params = {}\n",
    "        self.params['W_f'] = {}\n",
    "        self.params['B_f'] = {}\n",
    "        self.params['W_i'] = {}\n",
    "        self.params['B_i'] = {}\n",
    "        self.params['W_c'] = {}\n",
    "        self.params['B_c'] = {}\n",
    "        self.params['W_o'] = {}\n",
    "        self.params['B_o'] = {}        \n",
    "        self.params['W_v'] = {}\n",
    "        self.params['B_v'] = {}\n",
    "        \n",
    "        self.params['W_f']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size =(self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_f']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size=(1, self.hidden_size))\n",
    "        self.params['W_i']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size=(self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_i']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size=(1, self.hidden_size))\n",
    "        self.params['W_c']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size=(self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_c']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size=(1, self.hidden_size))\n",
    "        self.params['W_o']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size=(self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_o']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size=(1, self.hidden_size))       \n",
    "        self.params['W_v']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size=(self.hidden_size, self.output_size))\n",
    "        self.params['B_v']['value'] = np.random.normal(loc=0.0,scale=self.weight_scale,size=(1, self.output_size))\n",
    "        \n",
    "        #print(\"--------------------------------------------------------------------\")\n",
    "        #print(\"input_.shape\", input_.shape)\n",
    "        for key in self.params.keys():\n",
    "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['value'])\n",
    "        self.cells = [LSTMNode() for x in range(input_.shape[1])]\n",
    "\n",
    "\n",
    "    def _clear_gradients(self):\n",
    "        for key in self.params.keys():\n",
    "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['deriv'])\n",
    "                    \n",
    "        \n",
    "    def forward(self, x_seq_in):\n",
    "        self.first_backward= True\n",
    "        if self.first:\n",
    "            self._init_params(x_seq_in)\n",
    "            self.first=False\n",
    "        \n",
    "        batch_size = x_seq_in.shape[0]\n",
    "        \n",
    "        H_in = np.copy(self.start_H)\n",
    "        C_in = np.copy(self.start_C)\n",
    "        \n",
    "        H_in = np.repeat(H_in, batch_size, axis=0)\n",
    "        C_in = np.repeat(C_in, batch_size, axis=0)        \n",
    "\n",
    "        sequence_length = x_seq_in.shape[1]\n",
    "        \n",
    "        x_seq_out = np.zeros((batch_size, sequence_length, self.output_size))\n",
    "        \n",
    "        for t in range(sequence_length):\n",
    "            x_in = x_seq_in[:, t, :]\n",
    "            y_out, H_in, C_in = self.cells[t].forward(x_in, H_in, C_in, self.params)\n",
    "            x_seq_out[:, t, :] = y_out\n",
    "    \n",
    "        self.start_H = H_in.mean(axis=0, keepdims=True)\n",
    "        self.start_C = C_in.mean(axis=0, keepdims=True)        \n",
    "        \n",
    "        return x_seq_out, y_out\n",
    "\n",
    "\n",
    "    def backward(self, x_seq_out_grad):\n",
    "        batch_size = x_seq_out_grad.shape[0]\n",
    "        \n",
    "        h_in_grad = np.zeros((batch_size, self.hidden_size))\n",
    "        c_in_grad = np.zeros((batch_size, self.hidden_size))        \n",
    "        \n",
    "        num_chars = x_seq_out_grad.shape[1]\n",
    "        \n",
    "        x_seq_in_grad = np.zeros((batch_size, num_chars, self.vocab_size))\n",
    "        \n",
    "        for t in reversed(range(10)):\n",
    "            x_out_grad = x_seq_out_grad\n",
    "            grad_out, h_in_grad, c_in_grad = self.cells[t].backward(x_out_grad, h_in_grad, c_in_grad, self.params)\n",
    "            x_seq_in_grad[:, t, :] = grad_out\n",
    "        \n",
    "        return x_seq_in_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, layers, sequence_length, vocab_size, hidden_size,loss):\n",
    "        self.layers = layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.loss = loss\n",
    "        self.Linear = Linear(vocab_size, 5)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            setattr(layer, 'sequence_length', sequence_length)\n",
    "\n",
    "    def forward(self, x_batch):  \n",
    "        for layer in self.layers:\n",
    "            total, x_batch = layer.forward(x_batch)\n",
    "        #print(\"total\", total.shape)\n",
    "        #print(\"x_batch after LSTM\", x_batch.shape)\n",
    "        \n",
    "        x_batch = self.Linear.forward(x_batch)\n",
    "        #print(\"x_batch atfer Linear\", x_batch.shape)\n",
    "        return x_batch\n",
    "        \n",
    "    def backward(self, loss_grad):\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_grad = layer.backward(loss_grad)\n",
    "            \n",
    "        return loss_grad\n",
    "                \n",
    "    def single_step(self, x_batch, y_batch):\n",
    "\n",
    "        x_batch_out = self.forward(x_batch)\n",
    "        #print(\"x_batch_out\", x_batch_out.shape)\n",
    "        predict, _ = self.predict(x_batch_out, y_batch)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            layer._clear_gradients()\n",
    "            \n",
    "        loss = self.loss.forward(x_batch_out, y_batch)\n",
    "        loss_grad = self.loss.backward(x_batch_out, y_batch)\n",
    "        loss_grad = self.Linear.backward(loss_grad)\n",
    "        self.backward(loss_grad)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, X, Y):\n",
    "        pred = []\n",
    "        for probs in X:\n",
    "            pred.append(np.argmax(probs))\n",
    "        acc = 0\n",
    "        for i in range(len(Y)):\n",
    "            if pred[i] == Y[i]:\n",
    "                acc += 1\n",
    "        acc = acc/len(Y) * 100\n",
    "        return pred, acc\n",
    "    \n",
    "    def evaluate(self, x_batch, y_batch):\n",
    "        x_batch_out = self.forward(x_batch)\n",
    "        loss = self.loss.forward(x_batch_out, y_batch)\n",
    "        return loss\n",
    "    \n",
    "    def make_result(self, X, Y):\n",
    "        X_out = self.forward(X)\n",
    "        pred = []\n",
    "        for x in X_out:\n",
    "            pred.append(np.argmax(x))\n",
    "        acc = 0\n",
    "        for i in range(len(Y)):\n",
    "            if pred[i] == Y[i]:\n",
    "                acc += 1\n",
    "        acc = acc/len(Y) * 100\n",
    "        return pred, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTrainer:\n",
    "    def __init__(self, train_X, train_Y, test_X, test_Y, model,optim):\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y\n",
    "        self.model = model\n",
    "        self.sequence_length = self.model.sequence_length\n",
    "        self.optim = optim\n",
    "        self.train_loss_log = []\n",
    "        self.test_loss_log = []\n",
    "        self.log_step = []\n",
    "        setattr(self.optim, 'model', self.model)\n",
    "\n",
    "    def visualize_loss(self):\n",
    "        plt.plot(self.log_step, self.train_loss_log, label='train loss')\n",
    "        plt.plot(self.log_step, self.test_loss_log, label='validation loss')\n",
    "        plt.xlabel(\"num iteration\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.legend(fontsize='x-large')\n",
    "        \n",
    "    def train(self, num_iterations):\n",
    "        train_X = self.train_X\n",
    "        train_Y = self.train_Y\n",
    "        test_X = self.test_X\n",
    "        test_Y = self.test_Y \n",
    "        \n",
    "        num_iter = 0\n",
    "        \n",
    "        train_moving_average = deque(maxlen=100)\n",
    "        test_moving_average = deque(maxlen=100)\n",
    "        \n",
    "        while num_iter < num_iterations:\n",
    "            inputs_batch = np.array(train_X)\n",
    "            targets_batch = np.array(train_Y)\n",
    "            train_loss = self.model.single_step(inputs_batch, targets_batch)\n",
    "            test_loss = self.model.evaluate(np.array(test_X), np.array(test_Y))\n",
    "            train_moving_average.append(train_loss)\n",
    "            train_ma_loss = np.mean(train_moving_average)            \n",
    "            test_moving_average.append(test_loss)\n",
    "            test_ma_loss = np.mean(test_moving_average)\n",
    "            self.train_loss_log.append(train_ma_loss)\n",
    "            self.test_loss_log.append(test_ma_loss)\n",
    "            self.optim.step()\n",
    "            self.log_step.append(num_iter)\n",
    "            if num_iter % 100 == 0:\n",
    "                print(\"[Loss after %d iter] train loss: %f, test loss: %f\" % (num_iter, train_ma_loss, test_ma_loss))\n",
    "            num_iter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loss after 0 iter] train loss: 1.609305, test loss: 1.609078\n",
      "[Loss after 100 iter] train loss: 1.598969, test loss: 1.595938\n",
      "[Loss after 200 iter] train loss: 1.583173, test loss: 1.575568\n",
      "[Loss after 300 iter] train loss: 1.571858, test loss: 1.559044\n",
      "[Loss after 400 iter] train loss: 1.564172, test loss: 1.549108\n",
      "[Loss after 500 iter] train loss: 1.561366, test loss: 1.539577\n",
      "[Loss after 600 iter] train loss: 1.558818, test loss: 1.537595\n",
      "[Loss after 700 iter] train loss: 1.559308, test loss: 1.536057\n",
      "[Loss after 800 iter] train loss: 1.556420, test loss: 1.533544\n",
      "[Loss after 900 iter] train loss: 1.557501, test loss: 1.533512\n",
      "[Loss after 1000 iter] train loss: 1.558679, test loss: 1.533288\n",
      "[Loss after 1100 iter] train loss: 1.559163, test loss: 1.536166\n",
      "[Loss after 1200 iter] train loss: 1.557570, test loss: 1.533838\n",
      "[Loss after 1300 iter] train loss: 1.555422, test loss: 1.534165\n",
      "[Loss after 1400 iter] train loss: 1.556456, test loss: 1.533892\n",
      "[Loss after 1500 iter] train loss: 1.555987, test loss: 1.532652\n",
      "[Loss after 1600 iter] train loss: 1.553339, test loss: 1.531088\n",
      "[Loss after 1700 iter] train loss: 1.544736, test loss: 1.524598\n",
      "[Loss after 1800 iter] train loss: 1.457020, test loss: 1.464509\n",
      "[Loss after 1900 iter] train loss: 1.310959, test loss: 1.380585\n",
      "[Loss after 2000 iter] train loss: 1.223733, test loss: 1.297784\n",
      "[Loss after 2100 iter] train loss: 1.179319, test loss: 1.268834\n",
      "[Loss after 2200 iter] train loss: 1.143979, test loss: 1.245112\n",
      "[Loss after 2300 iter] train loss: 1.123047, test loss: 1.249781\n",
      "[Loss after 2400 iter] train loss: 1.107133, test loss: 1.268573\n",
      "[Loss after 2500 iter] train loss: 1.090842, test loss: 1.256837\n",
      "[Loss after 2600 iter] train loss: 1.077632, test loss: 1.274799\n",
      "[Loss after 2700 iter] train loss: 1.074342, test loss: 1.283278\n",
      "[Loss after 2800 iter] train loss: 1.055578, test loss: 1.254382\n",
      "[Loss after 2900 iter] train loss: 1.029695, test loss: 1.275460\n",
      "[Loss after 3000 iter] train loss: 0.953488, test loss: 1.262918\n",
      "[Loss after 3100 iter] train loss: 0.930455, test loss: 1.309354\n",
      "[Loss after 3200 iter] train loss: 0.779032, test loss: 1.239044\n",
      "[Loss after 3300 iter] train loss: 0.722587, test loss: 1.244654\n",
      "[Loss after 3400 iter] train loss: 0.675343, test loss: 1.185211\n",
      "[Loss after 3500 iter] train loss: 0.622031, test loss: 1.161370\n",
      "[Loss after 3600 iter] train loss: 0.553266, test loss: 1.121517\n",
      "[Loss after 3700 iter] train loss: 0.503240, test loss: 1.131415\n",
      "[Loss after 3800 iter] train loss: 0.500910, test loss: 1.125509\n",
      "[Loss after 3900 iter] train loss: 0.360148, test loss: 0.989364\n",
      "[Loss after 4000 iter] train loss: 0.356034, test loss: 1.033894\n",
      "[Loss after 4100 iter] train loss: 0.239588, test loss: 0.877240\n",
      "[Loss after 4200 iter] train loss: 0.187803, test loss: 0.876641\n",
      "[Loss after 4300 iter] train loss: 0.223326, test loss: 0.983432\n",
      "[Loss after 4400 iter] train loss: 0.110895, test loss: 0.992221\n",
      "[Loss after 4500 iter] train loss: 0.098684, test loss: 1.086980\n",
      "[Loss after 4600 iter] train loss: 0.092770, test loss: 1.129231\n",
      "[Loss after 4700 iter] train loss: 0.086723, test loss: 1.185127\n",
      "[Loss after 4800 iter] train loss: 0.500343, test loss: 1.189299\n",
      "[Loss after 4900 iter] train loss: 0.142729, test loss: 0.898692\n",
      "[Loss after 5000 iter] train loss: 0.159336, test loss: 1.004801\n"
     ]
    }
   ],
   "source": [
    "layers = [LSTMLayer(hidden_size=256, output_size=50, weight_scale=0.01)]\n",
    "model = Model(layers=layers, vocab_size=50, hidden_size=256, sequence_length=10,loss=SoftmaxCrossEntropy())\n",
    "optimizer = SGD(lr=0.02, gradient_clipping=True)\n",
    "RNN = RNNTrainer(embedded_train_X, train_Y, embedded_test_X, test_Y, model, optimizer)\n",
    "RNN.train(5001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_to_emoji(pred):\n",
    "    for i in pred:\n",
    "        print(utils.label_to_emoji(i), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN model accuracy for test set is 82.14%\n"
     ]
    }
   ],
   "source": [
    "pred, acc = RNN.model.make_result(np.array(embedded_test_X), test_Y)\n",
    "print(\"RNN model accuracy for test set is \"+ str(round(acc, 2))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🍴 😞 😄 😄 😄 😄 😞 😄 🍴 😄 ⚾ 😞 ❤️ 😞 ⚾ 😞 😞 😄 ⚾ 😄 😞 ⚾ 🍴 😞 😞 😞 ⚾ ❤️ ⚾ 😄 ❤️ ⚾ ❤️ 😄 😄 😞 😄 🍴 ⚾ 😄 ⚾ ❤️ ❤️ ⚾ 😄 😄 😄 😄 😞 😞 😞 ❤️ 😞 😄 😄 😄 "
     ]
    }
   ],
   "source": [
    "pred_to_emoji(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "RNN.visualize_loss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
